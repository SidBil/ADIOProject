{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune Whisper Small on TORGO with LoRA\n",
    "\n",
    "This notebook fine-tunes **Whisper Small** on the preprocessed TORGO dataset using **LoRA** (Low-Rank Adaptation) for dysarthric speech recognition.\n",
    "\n",
    "**Supports both Google Colab (GPU) and local (MPS/CPU).**\n",
    "\n",
    "### Colab Setup\n",
    "1. Upload your `audio/torgo/processed/` folder to Google Drive under `My Drive/ASR Project/`\n",
    "   - The folder should contain `train/`, `validation/`, `test/` subfolders with `.wav` files\n",
    "   - Also include `metadata.json` inside `processed/`\n",
    "2. Select a **T4 GPU** runtime: Runtime \u2192 Change runtime type \u2192 T4 GPU\n",
    "3. Run all cells\n",
    "\n",
    "### Local Setup\n",
    "1. Run `audio/data_loader.ipynb` to download TORGO\n",
    "2. Run `audio/torgo_preprocessing.ipynb` to generate processed audio + metadata\n",
    "3. Run this notebook from the **project root**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os, sys\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Running on Google Colab\")\n",
    "    !pip install -q transformers datasets evaluate peft accelerate jiwer soundfile librosa\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    PROJECT_ROOT = \"/content\"\n",
    "    DATA_ROOT = \"/content/drive/MyDrive/ASR Project\"\n",
    "    os.makedirs(PROJECT_ROOT, exist_ok=True)\n",
    "else:\n",
    "    print(\"Running locally\")\n",
    "    from pathlib import Path\n",
    "    PROJECT_ROOT = str(Path(os.getcwd()).resolve())\n",
    "    while PROJECT_ROOT != \"/\" and not os.path.exists(os.path.join(PROJECT_ROOT, \"asr\", \"config.yaml\")):\n",
    "        PROJECT_ROOT = str(Path(PROJECT_ROOT).parent)\n",
    "    os.chdir(PROJECT_ROOT)\n",
    "    DATA_ROOT = PROJECT_ROOT\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Data root:    {DATA_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "\n",
    "if IN_COLAB:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "else:\n",
    "    device = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "config = {\n",
    "    \"model\": {\n",
    "        \"name\": \"openai/whisper-small\",\n",
    "        \"language\": \"en\",\n",
    "        \"task\": \"transcribe\",\n",
    "    },\n",
    "    \"lora\": {\n",
    "        \"r\": 16,\n",
    "        \"alpha\": 32,\n",
    "        \"dropout\": 0.1,\n",
    "        \"target_modules\": [\"q_proj\", \"v_proj\", \"k_proj\", \"out_proj\"],\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"processed_dir\": os.path.join(DATA_ROOT, \"audio\", \"torgo\", \"processed\"),\n",
    "        \"metadata_path\": os.path.join(DATA_ROOT, \"audio\", \"torgo\", \"processed\", \"metadata.json\"),\n",
    "        \"sampling_rate\": 16000,\n",
    "        \"max_audio_length\": 15.0,\n",
    "        \"include_augmented\": True,\n",
    "        \"dysarthric_only\": False,\n",
    "    },\n",
    "    \"output\": {\n",
    "        \"model_dir\": os.path.join(PROJECT_ROOT, \"asr_checkpoints\", \"whisper-lora\"),\n",
    "        \"log_dir\": os.path.join(PROJECT_ROOT, \"asr_runs\"),\n",
    "    },\n",
    "}\n",
    "\n",
    "if device == \"cuda\":\n",
    "    train_cfg = {\n",
    "        \"epochs\": 2,\n",
    "        \"batch_size\": 8,\n",
    "        \"gradient_accumulation_steps\": 2,\n",
    "        \"learning_rate\": 3e-4,\n",
    "        \"warmup_steps\": 50,\n",
    "        \"fp16\": True,\n",
    "        \"eval_steps\": 500,\n",
    "        \"save_steps\": 500,\n",
    "        \"logging_steps\": 25,\n",
    "        \"dataloader_num_workers\": 2,\n",
    "    }\n",
    "elif device == \"mps\":\n",
    "    train_cfg = {\n",
    "        \"epochs\": 2,\n",
    "        \"batch_size\": 1,\n",
    "        \"gradient_accumulation_steps\": 16,\n",
    "        \"learning_rate\": 3e-4,\n",
    "        \"warmup_steps\": 50,\n",
    "        \"fp16\": False,\n",
    "        \"eval_steps\": 250,\n",
    "        \"save_steps\": 250,\n",
    "        \"logging_steps\": 25,\n",
    "        \"dataloader_num_workers\": 0,\n",
    "    }\n",
    "else:\n",
    "    train_cfg = {\n",
    "        \"epochs\": 2,\n",
    "        \"batch_size\": 2,\n",
    "        \"gradient_accumulation_steps\": 8,\n",
    "        \"learning_rate\": 3e-4,\n",
    "        \"warmup_steps\": 50,\n",
    "        \"fp16\": False,\n",
    "        \"eval_steps\": 250,\n",
    "        \"save_steps\": 250,\n",
    "        \"logging_steps\": 25,\n",
    "        \"dataloader_num_workers\": 0,\n",
    "    }\n",
    "\n",
    "effective_batch = train_cfg[\"batch_size\"] * train_cfg[\"gradient_accumulation_steps\"]\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Batch: {train_cfg['batch_size']} x {train_cfg['gradient_accumulation_steps']} = {effective_batch} effective\")\n",
    "print(f\"FP16: {train_cfg['fp16']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Preprocessed Data\n",
    "\n",
    "Reads the metadata JSON and constructs a HuggingFace Dataset from the processed `.wav` files. Audio is loaded lazily (only decoded when accessed), so this step is fast and lightweight."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json, re\n",
    "from datasets import Dataset, DatasetDict, Audio\n",
    "\n",
    "data_cfg = config[\"data\"]\n",
    "processed_dir = data_cfg[\"processed_dir\"]\n",
    "metadata_path = data_cfg[\"metadata_path\"]\n",
    "\n",
    "assert os.path.exists(metadata_path), f\"Metadata not found at {metadata_path}\"\n",
    "\n",
    "with open(metadata_path) as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "include_augmented = data_cfg[\"include_augmented\"]\n",
    "dysarthric_only = data_cfg[\"dysarthric_only\"]\n",
    "AUGMENT_PATTERN = re.compile(r\"sample_\\d{5}_.+\\.wav\")\n",
    "\n",
    "def load_split(split_name):\n",
    "    split_meta = metadata[split_name]\n",
    "    audio_paths, transcriptions = [], []\n",
    "    for filename, meta in split_meta.items():\n",
    "        if dysarthric_only and meta[\"speech_status\"] != \"dysarthria\":\n",
    "            continue\n",
    "        if not include_augmented and AUGMENT_PATTERN.match(filename):\n",
    "            continue\n",
    "        wav_path = os.path.join(processed_dir, split_name, filename)\n",
    "        if os.path.exists(wav_path) and meta[\"transcription\"]:\n",
    "            audio_paths.append(wav_path)\n",
    "            transcriptions.append(meta[\"transcription\"])\n",
    "    ds = Dataset.from_dict({\"audio\": audio_paths, \"transcription\": transcriptions})\n",
    "    ds = ds.cast_column(\"audio\", Audio(sampling_rate=data_cfg[\"sampling_rate\"]))\n",
    "    return ds\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": load_split(\"train\"),\n",
    "    \"validation\": load_split(\"validation\"),\n",
    "    \"test\": load_split(\"test\"),\n",
    "})\n",
    "\n",
    "print(f\"Augmented: {'included' if include_augmented else 'excluded'}\")\n",
    "print(f\"Filter:   {'dysarthric only' if dysarthric_only else 'all speakers'}\")\n",
    "print(f\"Train: {len(dataset['train']):,}  Val: {len(dataset['validation']):,}  Test: {len(dataset['test']):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import AutoProcessor\n",
    "\n",
    "model_name = config[\"model\"][\"name\"]\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    model_name,\n",
    "    language=config[\"model\"][\"language\"],\n",
    "    task=config[\"model\"][\"task\"],\n",
    ")\n",
    "print(\"Processor ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On-the-fly Feature Extraction\n",
    "\n",
    "Instead of precomputing Whisper mel spectrograms for all ~92K samples (which requires ~88 GB of storage and RAM), the data collator extracts features **on-the-fly** during training. Only one batch of features is ever in memory at a time."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Any\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: list[dict]) -> dict[str, torch.Tensor]:\n",
    "        input_features = []\n",
    "        for f in features:\n",
    "            audio = f[\"audio\"]\n",
    "            mel = self.processor.feature_extractor(\n",
    "                audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]\n",
    "            ).input_features[0]\n",
    "            input_features.append({\"input_features\": mel})\n",
    "\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        label_features = [\n",
    "            {\"input_ids\": self.processor.tokenizer(f[\"transcription\"]).input_ids}\n",
    "            for f in features\n",
    "        ]\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
    "            labels_batch.attention_mask.ne(1), -100\n",
    "        )\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
    "print(\"Data collator ready (on-the-fly feature extraction).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup\n",
    "\n",
    "Load Whisper Small and apply LoRA adapters. Gradient checkpointing trades compute for memory \u2014 essential for fitting on consumer GPUs."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []\n",
    "\n",
    "lora_cfg = config[\"lora\"]\n",
    "lora_config = LoraConfig(\n",
    "    r=lora_cfg[\"r\"],\n",
    "    lora_alpha=lora_cfg[\"alpha\"],\n",
    "    target_modules=lora_cfg[\"target_modules\"],\n",
    "    lora_dropout=lora_cfg[\"dropout\"],\n",
    "    bias=\"none\",\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.gradient_checkpointing_enable()\n",
    "model.print_trainable_parameters()\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "    label_ids = np.where(label_ids != -100, label_ids, processor.tokenizer.pad_token_id)\n",
    "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The trainer handles device placement, gradient accumulation, mixed precision, evaluation, and checkpointing automatically."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, TrainerCallback, TrainerState, TrainerControl, TrainingArguments, EarlyStoppingCallback\n",
    "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
    "\n",
    "class SavePeftModelCallback(TrainerCallback):\n",
    "    def on_save(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        checkpoint_folder = os.path.join(args.output_dir, f\"{PREFIX_CHECKPOINT_DIR}-{state.global_step}\")\n",
    "        peft_model_path = os.path.join(checkpoint_folder, \"adapter_model\")\n",
    "        kwargs[\"model\"].save_pretrained(peft_model_path)\n",
    "        pytorch_bin = os.path.join(checkpoint_folder, \"pytorch_model.bin\")\n",
    "        if os.path.exists(pytorch_bin):\n",
    "            os.remove(pytorch_bin)\n",
    "        return control\n",
    "\n",
    "output_cfg = config[\"output\"]\n",
    "os.makedirs(output_cfg[\"model_dir\"], exist_ok=True)\n",
    "os.makedirs(output_cfg[\"log_dir\"], exist_ok=True)\n",
    "\n",
    "steps_per_epoch = len(dataset[\"train\"]) // effective_batch\n",
    "total_steps = steps_per_epoch * train_cfg[\"epochs\"]\n",
    "print(f\"Steps/epoch: ~{steps_per_epoch:,}  |  Total: ~{total_steps:,}\")\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_cfg[\"model_dir\"],\n",
    "    logging_dir=output_cfg[\"log_dir\"],\n",
    "    num_train_epochs=train_cfg[\"epochs\"],\n",
    "    per_device_train_batch_size=train_cfg[\"batch_size\"],\n",
    "    per_device_eval_batch_size=train_cfg[\"batch_size\"],\n",
    "    learning_rate=train_cfg[\"learning_rate\"],\n",
    "    warmup_steps=train_cfg[\"warmup_steps\"],\n",
    "    fp16=train_cfg[\"fp16\"],\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=train_cfg[\"eval_steps\"],\n",
    "    save_steps=train_cfg[\"save_steps\"],\n",
    "    gradient_accumulation_steps=train_cfg[\"gradient_accumulation_steps\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_wer\",\n",
    "    greater_is_better=False,\n",
    "    generation_max_length=225,\n",
    "    logging_steps=train_cfg[\"logging_steps\"],\n",
    "    remove_unused_columns=False,\n",
    "    label_names=[\"labels\"],\n",
    "    save_total_limit=3,\n",
    "    dataloader_pin_memory=(device == \"cuda\"),\n",
    "    dataloader_num_workers=train_cfg[\"dataloader_num_workers\"],\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    processing_class=processor.feature_extractor,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[\n",
    "        SavePeftModelCallback(),\n",
    "        EarlyStoppingCallback(early_stopping_patience=3),\n",
    "    ],\n",
    ")\n",
    "model.config.use_cache = False\n",
    "print(\"Trainer ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "final_dir = os.path.join(output_cfg[\"model_dir\"], \"final_adapter\")\n",
    "os.makedirs(final_dir, exist_ok=True)\n",
    "trainer.save_model(final_dir)\n",
    "print(f\"Saved final LoRA adapter to {final_dir}\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    drive_save = os.path.join(DATA_ROOT, \"asr_checkpoints\", \"whisper-lora-final\")\n",
    "    os.makedirs(drive_save, exist_ok=True)\n",
    "    import shutil\n",
    "    shutil.copytree(final_dir, drive_save, dirs_exist_ok=True)\n",
    "    print(f\"Copied to Google Drive: {drive_save}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}