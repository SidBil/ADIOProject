{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8a04889a",
      "metadata": {
        "id": "8a04889a"
      },
      "source": [
        "# TORGO `baseline_evaluation.py` — Colab Notebook (Explained)\n",
        "\n",
        "## What this `.py` file is for\n",
        "The script evaluates **pretrained Whisper** models on the TORGO dataset and reports:\n",
        "- **WER** (Word Error Rate) and **CER** (Character Error Rate)\n",
        "- Metrics broken down by **speech status** (e.g., dysarthric vs healthy)\n",
        "- Error breakdown counts: **substitutions**, **deletions**, **insertions**\n",
        "- A summary table comparing multiple model sizes (tiny/base/small)\n",
        "\n",
        "### Key definitions\n",
        "- **Reference (ref)**: the ground-truth transcription (what was actually said)\n",
        "- **Hypothesis (hyp)**: the model’s predicted transcription\n",
        "- **WER (Word Error Rate)**:  \n",
        "  \\[ \\text{WER} = \\frac{S + D + I}{N} \\]  \n",
        "  where `S`=substitutions, `D`=deletions, `I`=insertions, `N`=number of reference words.\n",
        "- **CER (Character Error Rate)**: same idea as WER but measured at the character level.\n",
        "\n",
        "> In plain language: WER tells you “how many word-level edits it takes” to turn the model output into the correct text.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a289128",
      "metadata": {
        "id": "6a289128"
      },
      "source": [
        "## 0) Setup: Imports and model registry\n",
        "\n",
        "### Why these imports exist\n",
        "- `torch`: runs Whisper inference and chooses device (cuda/mps/cpu)\n",
        "- `numpy`: audio arrays are NumPy arrays\n",
        "- `jiwer`: computes WER/CER + word-level error breakdown\n",
        "- `transformers`: loads Whisper model + processor\n",
        "- `datasets`: loads saved TORGO dataset from disk and decodes audio\n",
        "- `argparse/json/pathlib/defaultdict`: CLI + saving results + grouping by status\n",
        "\n",
        "### `WHISPER_MODELS`\n",
        "This dictionary maps a short name (e.g. `\"tiny\"`) to the Hugging Face model id.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "447c44c7",
      "metadata": {
        "id": "447c44c7"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import json\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from jiwer import wer, cer, process_words\n",
        "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
        "from datasets import load_from_disk, Audio\n",
        "\n",
        "\n",
        "WHISPER_MODELS = {\n",
        "    \"tiny\": \"openai/whisper-tiny\",\n",
        "    \"base\": \"openai/whisper-base\",\n",
        "    \"small\": \"openai/whisper-small\",\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58eb7c6e",
      "metadata": {
        "id": "58eb7c6e"
      },
      "source": [
        "## 1) `transcribe_audio(model, processor, audio_array, sr, device)`\n",
        "\n",
        "### Purpose\n",
        "Transcribe **one audio sample** using a Whisper model.\n",
        "\n",
        "### Step-by-step\n",
        "1. Use `processor(...)` to convert the raw audio waveform (`audio_array`) into Whisper input features.\n",
        "2. Move features onto the selected device (`cpu`, `cuda`, or `mps`).\n",
        "3. Disable gradients with `torch.no_grad()` (faster and uses less memory for inference).\n",
        "4. Use `model.generate(...)` to produce token ids for the transcription.\n",
        "5. Decode tokens back to text with `processor.batch_decode(...)`.\n",
        "6. Normalize output by stripping whitespace and lowercasing.\n",
        "\n",
        "### Why lowercasing?\n",
        "WER is sensitive to casing differences unless you normalize. Lowercasing makes metrics more consistent.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c85dae11",
      "metadata": {
        "id": "c85dae11"
      },
      "outputs": [],
      "source": [
        "def transcribe_audio(model, processor, audio_array: np.ndarray, sr: int, device: str) -> str:\n",
        "    \"\"\"Transcribe a single audio sample using Whisper.\"\"\"\n",
        "    input_features = processor(\n",
        "        audio_array, sampling_rate=sr, return_tensors=\"pt\"\n",
        "    ).input_features.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        predicted_ids = model.generate(input_features)\n",
        "\n",
        "    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
        "    return transcription.strip().lower()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8bd7bfd",
      "metadata": {
        "id": "d8bd7bfd"
      },
      "source": [
        "## 2) `evaluate_model(model_name, model_id, dataset, device)`\n",
        "\n",
        "### Purpose\n",
        "Evaluate **one Whisper model** on the **test split** and compute metrics:\n",
        "- WER/CER overall\n",
        "- WER/CER per `speech_status`\n",
        "- Error breakdown counts per group\n",
        "\n",
        "### Key steps\n",
        "1. Load Whisper processor + model from `model_id`\n",
        "2. Iterate over the **test split**\n",
        "3. For each sample:\n",
        "   - Get `reference` transcription (ground truth)\n",
        "   - Get `speech_status` group label\n",
        "   - Get audio waveform (`audio[\"array\"]`) and sample rate (`audio[\"sampling_rate\"]`)\n",
        "   - Produce `hypothesis` transcription via `transcribe_audio(...)`\n",
        "   - Append reference/hypothesis into a group bucket (`group_results[status]`)\n",
        "4. After the loop, compute metrics for each group using `jiwer`:\n",
        "   - `wer(refs, hyps)`\n",
        "   - `cer(refs, hyps)`\n",
        "   - `process_words(refs, hyps)` gives substitutions/deletions/insertions counts\n",
        "5. Also compute **overall** WER/CER by aggregating all refs/hyps\n",
        "\n",
        "### Why group by `speech_status`?\n",
        "Because dysarthric speech is harder for ASR. You want to see how performance differs on dysarthric vs healthy speech.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "2a567fac",
      "metadata": {
        "id": "2a567fac"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model_name: str, model_id: str, dataset, device: str) -> dict:\n",
        "    \"\"\"Evaluate a single Whisper model on the test split.\"\"\"\n",
        "    print(f\"\\nEvaluating {model_name} ({model_id})...\")\n",
        "\n",
        "    processor = WhisperProcessor.from_pretrained(model_id)\n",
        "    model = WhisperForConditionalGeneration.from_pretrained(model_id).to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Collect results grouped by speech_status\n",
        "    group_results = defaultdict(lambda: {\"refs\": [], \"hyps\": []})\n",
        "\n",
        "    test_data = dataset[\"test\"] if \"test\" in dataset else dataset[list(dataset.keys())[0]]\n",
        "    total = len(test_data)\n",
        "\n",
        "    for i, sample in enumerate(test_data):\n",
        "        reference = sample.get(\"transcription\", \"\")\n",
        "        if not reference:\n",
        "            continue\n",
        "        reference = reference.strip().lower()\n",
        "\n",
        "        status = sample.get(\"speech_status\", \"unknown\")\n",
        "        # Handle ClassLabel encoding (int -> string)\n",
        "        if isinstance(status, int):\n",
        "            status_map = {0: \"dysarthric\", 1: \"healthy\"}\n",
        "            status = status_map.get(status, f\"unknown_{status}\")\n",
        "        audio = sample[\"audio\"]\n",
        "\n",
        "        hypothesis = transcribe_audio(\n",
        "            model, processor, audio[\"array\"], audio[\"sampling_rate\"], device\n",
        "        )\n",
        "\n",
        "        group_results[status][\"refs\"].append(reference)\n",
        "        group_results[status][\"hyps\"].append(hypothesis)\n",
        "\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(f\"  Processed {i + 1}/{total} samples...\")\n",
        "\n",
        "    # Compute metrics\n",
        "    report = {\"model\": model_name, \"model_id\": model_id, \"groups\": {}}\n",
        "\n",
        "    all_refs, all_hyps = [], []\n",
        "\n",
        "    for status, data in group_results.items():\n",
        "        refs, hyps = data[\"refs\"], data[\"hyps\"]\n",
        "        all_refs.extend(refs)\n",
        "        all_hyps.extend(hyps)\n",
        "\n",
        "        group_wer = wer(refs, hyps)\n",
        "        group_cer = cer(refs, hyps)\n",
        "\n",
        "        # Error type breakdown\n",
        "        output = process_words(refs, hyps)\n",
        "        report[\"groups\"][status] = {\n",
        "            \"wer\": group_wer,\n",
        "            \"cer\": group_cer,\n",
        "            \"substitutions\": output.substitutions,\n",
        "            \"deletions\": output.deletions,\n",
        "            \"insertions\": output.insertions,\n",
        "            \"num_samples\": len(refs),\n",
        "        }\n",
        "\n",
        "    # Overall metrics\n",
        "    if all_refs:\n",
        "        report[\"overall_wer\"] = wer(all_refs, all_hyps)\n",
        "        report[\"overall_cer\"] = cer(all_refs, all_hyps)\n",
        "        report[\"total_samples\"] = len(all_refs)\n",
        "\n",
        "    print(f\"  Done. Overall WER: {report.get('overall_wer', 0)*100:.1f}%\")\n",
        "    return report\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8dca1e11",
      "metadata": {
        "id": "8dca1e11"
      },
      "source": [
        "## 3) `print_report(reports)`\n",
        "\n",
        "### Purpose\n",
        "Pretty-print a comparison table for multiple evaluated models.\n",
        "\n",
        "### What it prints\n",
        "1. A header for results\n",
        "2. A table with columns:\n",
        "   - Model name\n",
        "   - Overall WER\n",
        "   - Overall CER\n",
        "   - Dysarthric WER\n",
        "   - Healthy WER\n",
        "3. The **best model** (lowest overall WER)\n",
        "4. Per-group details for the best model:\n",
        "   - WER, CER\n",
        "   - number of samples\n",
        "   - substitutions, deletions, insertions\n",
        "5. The WER gap between dysarthric and healthy speech\n",
        "\n",
        "### Why choose “best” by overall WER?\n",
        "WER is the most common headline metric for ASR quality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "746d7967",
      "metadata": {
        "id": "746d7967"
      },
      "outputs": [],
      "source": [
        "def print_report(reports: list[dict]):\n",
        "    \"\"\"Print formatted comparison of all models.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"BASELINE EVALUATION RESULTS\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Model comparison\n",
        "    print(f\"\\n{'Model':>10} {'Overall WER':>12} {'Overall CER':>12} {'Dysarthric':>12} {'Healthy':>12}\")\n",
        "    print(\"-\" * 60)\n",
        "    for r in reports:\n",
        "        dys_wer = r.get(\"groups\", {}).get(\"dysarthric\", {}).get(\"wer\", 0)\n",
        "        healthy_wer = r.get(\"groups\", {}).get(\"healthy\", {}).get(\"wer\", 0)\n",
        "        print(\n",
        "            f\"{r['model']:>10} \"\n",
        "            f\"{r.get('overall_wer', 0)*100:>11.1f}% \"\n",
        "            f\"{r.get('overall_cer', 0)*100:>11.1f}% \"\n",
        "            f\"{dys_wer*100:>11.1f}% \"\n",
        "            f\"{healthy_wer*100:>11.1f}%\"\n",
        "        )\n",
        "\n",
        "    # Best model\n",
        "    best = min(reports, key=lambda r: r.get(\"overall_wer\", float(\"inf\")))\n",
        "    print(f\"\\nBest model: {best['model']} (WER: {best.get('overall_wer', 0)*100:.1f}%)\")\n",
        "\n",
        "    # Per-group details for best model\n",
        "    print(f\"\\n{'Per-Group Details (best model: ' + best['model'] + ')':=^70}\")\n",
        "    print(f\"  {'Group':<14} {'WER':>8} {'CER':>8} {'Samples':>8} {'Sub':>6} {'Del':>6} {'Ins':>6}\")\n",
        "    print(\"  \" + \"-\" * 56)\n",
        "    for group, data in sorted(best[\"groups\"].items()):\n",
        "        print(\n",
        "            f\"  {group:<14} \"\n",
        "            f\"{data['wer']*100:>7.1f}% \"\n",
        "            f\"{data['cer']*100:>7.1f}% \"\n",
        "            f\"{data['num_samples']:>8} \"\n",
        "            f\"{data['substitutions']:>6} \"\n",
        "            f\"{data['deletions']:>6} \"\n",
        "            f\"{data['insertions']:>6}\"\n",
        "        )\n",
        "\n",
        "    # WER gap\n",
        "    dys = best[\"groups\"].get(\"dysarthric\", {}).get(\"wer\", 0)\n",
        "    healthy = best[\"groups\"].get(\"healthy\", {}).get(\"wer\", 0)\n",
        "    if dys and healthy:\n",
        "        print(f\"\\n  WER gap (dysarthric - healthy): {(dys - healthy)*100:.1f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07966ef8",
      "metadata": {
        "id": "07966ef8"
      },
      "source": [
        "## 4) `main()` — orchestrates the full evaluation\n",
        "\n",
        "### Purpose\n",
        "Turn the evaluation functions into a command-line tool that:\n",
        "1. Loads a local saved TORGO dataset (`load_from_disk`)\n",
        "2. Evaluates multiple Whisper model sizes\n",
        "3. Prints a summary report\n",
        "4. Saves results to JSON\n",
        "\n",
        "### Key steps\n",
        "1. Parse CLI args:\n",
        "   - `--models`: which Whisper sizes to evaluate\n",
        "   - `--input`: directory containing `torgo_dataset/`\n",
        "   - `--output`: where to save JSON results\n",
        "2. Pick `device`:\n",
        "   - `cuda` if GPU is available\n",
        "   - otherwise `mps` (Apple Silicon) if available\n",
        "   - otherwise `cpu`\n",
        "3. Load dataset and cast audio to 16kHz\n",
        "4. Evaluate each requested model\n",
        "5. Print comparison report and save results\n",
        "\n",
        "### In Colab\n",
        "Instead of CLI args, you’ll usually set Python variables.  \n",
        "So we also provide a notebook runner at the end.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "5f0e5858",
      "metadata": {
        "id": "5f0e5858"
      },
      "outputs": [],
      "source": [
        "args = argparse.Namespace(\n",
        "    models=[\"tiny\", \"base\", \"small\"],\n",
        "    input=\"../audio/torgo\",\n",
        "    output=\"./baseline_results.json\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "8419feff",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: mps\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "208848ed",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading dataset from ../audio/torgo/torgo_dataset...\n"
          ]
        }
      ],
      "source": [
        "dataset_path = Path(args.input) / \"torgo_dataset\"\n",
        "print(f\"Loading dataset from {dataset_path}...\")\n",
        "dataset = load_from_disk(str(dataset_path))\n",
        "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "429894ed",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating tiny (openai/whisper-tiny)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "Loading weights: 100%|██████████| 167/167 [00:00<00:00, 1813.17it/s, Materializing param=model.encoder.layers.3.self_attn_layer_norm.weight]  \n",
            "Using custom `forced_decoder_ids` from the (generation) config. This is deprecated in favor of the `task` and `language` flags/config options.\n",
            "Transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English. This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`. See https://github.com/huggingface/transformers/pull/28687 for more details.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "A custom logits processor of type <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> has been passed to `.generate()`, but it was also created in `.generate()`, given its parameterization. The custom <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> will take precedence. Please check the docstring of <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> to see related `.generate()` flags.\n",
            "A custom logits processor of type <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> has been passed to `.generate()`, but it was also created in `.generate()`, given its parameterization. The custom <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> will take precedence. Please check the docstring of <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> to see related `.generate()` flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 100/1656 samples...\n",
            "  Processed 200/1656 samples...\n",
            "  Processed 300/1656 samples...\n",
            "  Processed 400/1656 samples...\n",
            "  Processed 500/1656 samples...\n",
            "  Processed 600/1656 samples...\n",
            "  Processed 700/1656 samples...\n",
            "  Processed 800/1656 samples...\n",
            "  Processed 900/1656 samples...\n",
            "  Processed 1000/1656 samples...\n",
            "  Processed 1100/1656 samples...\n",
            "  Processed 1200/1656 samples...\n",
            "  Processed 1300/1656 samples...\n",
            "  Processed 1400/1656 samples...\n",
            "  Processed 1500/1656 samples...\n",
            "  Processed 1600/1656 samples...\n",
            "  Done. Overall WER: 82.6%\n",
            "\n",
            "Evaluating base (openai/whisper-base)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading weights: 100%|██████████| 245/245 [00:00<00:00, 1820.14it/s, Materializing param=model.encoder.layers.5.self_attn_layer_norm.weight]   \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 100/1656 samples...\n",
            "  Processed 200/1656 samples...\n",
            "  Processed 300/1656 samples...\n",
            "  Processed 400/1656 samples...\n",
            "  Processed 500/1656 samples...\n",
            "  Processed 600/1656 samples...\n",
            "  Processed 700/1656 samples...\n",
            "  Processed 800/1656 samples...\n",
            "  Processed 900/1656 samples...\n",
            "  Processed 1000/1656 samples...\n",
            "  Processed 1100/1656 samples...\n",
            "  Processed 1200/1656 samples...\n",
            "  Processed 1300/1656 samples...\n",
            "  Processed 1400/1656 samples...\n",
            "  Processed 1500/1656 samples...\n",
            "  Processed 1600/1656 samples...\n",
            "  Done. Overall WER: 99.5%\n",
            "\n",
            "Evaluating small (openai/whisper-small)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading weights: 100%|██████████| 479/479 [00:00<00:00, 1917.75it/s, Materializing param=model.encoder.layers.11.self_attn_layer_norm.weight]   \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed 100/1656 samples...\n",
            "  Processed 200/1656 samples...\n",
            "  Processed 300/1656 samples...\n",
            "  Processed 400/1656 samples...\n",
            "  Processed 500/1656 samples...\n",
            "  Processed 600/1656 samples...\n",
            "  Processed 700/1656 samples...\n",
            "  Processed 800/1656 samples...\n",
            "  Processed 900/1656 samples...\n",
            "  Processed 1000/1656 samples...\n",
            "  Processed 1100/1656 samples...\n",
            "  Processed 1200/1656 samples...\n",
            "  Processed 1300/1656 samples...\n",
            "  Processed 1400/1656 samples...\n",
            "  Processed 1500/1656 samples...\n",
            "  Processed 1600/1656 samples...\n",
            "  Done. Overall WER: 61.8%\n",
            "\n",
            "======================================================================\n",
            "BASELINE EVALUATION RESULTS\n",
            "======================================================================\n",
            "\n",
            "     Model  Overall WER  Overall CER   Dysarthric      Healthy\n",
            "------------------------------------------------------------\n",
            "      tiny        82.6%        63.7%       153.8%        44.4%\n",
            "      base        99.5%        74.0%       192.5%        49.5%\n",
            "     small        61.8%        51.3%       111.6%        35.1%\n",
            "\n",
            "Best model: small (WER: 61.8%)\n",
            "\n",
            "================Per-Group Details (best model: small)=================\n",
            "  Group               WER      CER  Samples    Sub    Del    Ins\n",
            "  --------------------------------------------------------\n",
            "  dysarthric       111.6%   126.3%      558    806     95    810\n",
            "  healthy           35.1%    11.6%     1098    959     11     30\n",
            "\n",
            "  WER gap (dysarthric - healthy): 76.5%\n"
          ]
        }
      ],
      "source": [
        "reports = []\n",
        "for model_name in args.models:\n",
        "    report = evaluate_model(model_name, WHISPER_MODELS[model_name], dataset, device)\n",
        "    reports.append(report)\n",
        "\n",
        "print_report(reports)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "ce817fe7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Results saved to baseline_results.json\n"
          ]
        }
      ],
      "source": [
        "# Save results\n",
        "output_path = Path(args.output)\n",
        "with open(output_path, \"w\") as f:\n",
        "    json.dump(reports, f, indent=2)\n",
        "print(f\"\\nResults saved to {output_path}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [
        {
          "file_id": "1gI9-Qofb3lcDWO3pJGjjXlMckhS6RkcG",
          "timestamp": 1771045874376
        }
      ]
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
