{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multimodal Analysis — Where Does CLIP Help vs Hurt?\n",
        "\n",
        "This notebook compares **ASR-only** (α=0) against **multimodal rescoring** (α>0)\n",
        "at the individual sample level, then aggregates to answer the key questions:\n",
        "\n",
        "- In what **percentage of cases** does multimodal rescoring improve WER?\n",
        "- Is the benefit concentrated in **dysarthric** speech, **healthy** speech, or both?\n",
        "- Which **specific samples** see the biggest improvement or degradation?\n",
        "- Does the system meet the **success criterion**: multimodal helps ≥ 50% of test cases?\n",
        "\n",
        "## Why this matters\n",
        "\n",
        "CLIP rescoring doesn't always help. If the ASR already got the right answer, CLIP\n",
        "can potentially push a worse hypothesis to the top. Understanding *when* and *why*\n",
        "multimodal helps is critical for deciding whether to deploy it and at what α."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "376cb797",
      "metadata": {},
      "source": [
        "## 0) Imports and prerequisite code\n",
        "\n",
        "This notebook is **self-contained** — it includes the full `MultimodalASR` pipeline\n",
        "inline so it can run independently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "396ef5b3",
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import soundfile as sf\n",
        "from PIL import Image\n",
        "from jiwer import wer as compute_wer, cer as compute_cer\n",
        "from transformers import (\n",
        "    WhisperProcessor,\n",
        "    WhisperForConditionalGeneration,\n",
        "    CLIPModel,\n",
        "    CLIPProcessor,\n",
        ")\n",
        "\n",
        "# ── Prerequisite code (defined in clip_embeddings & multimodal_asr notebooks) ──\n",
        "# Included inline so this notebook is self-contained and runnable on its own.\n",
        "\n",
        "DEFAULT_CLIP_MODEL = \"openai/clip-vit-base-patch32\"\n",
        "DEFAULT_WHISPER_MODEL = \"openai/whisper-small\"\n",
        "DEFAULT_CACHE_PATH = Path(\"cache/clip_image_embeddings.npz\")\n",
        "\n",
        "def _pick_device():\n",
        "    if torch.cuda.is_available():\n",
        "        return \"cuda\"\n",
        "    if torch.backends.mps.is_available():\n",
        "        return \"mps\"\n",
        "    return \"cpu\"\n",
        "\n",
        "def load_clip(model_name=DEFAULT_CLIP_MODEL, device=None):\n",
        "    if device is None:\n",
        "        device = _pick_device()\n",
        "    processor = CLIPProcessor.from_pretrained(model_name)\n",
        "    model = CLIPModel.from_pretrained(model_name).to(device)\n",
        "    model.eval()\n",
        "    return model, processor, device\n",
        "\n",
        "def _get_image_features(model, pixel_values):\n",
        "    vision_out = model.vision_model(pixel_values=pixel_values)\n",
        "    features = model.visual_projection(vision_out.pooler_output)\n",
        "    return features / features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "def _get_text_features(model, input_ids, attention_mask):\n",
        "    text_out = model.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    features = model.text_projection(text_out.pooler_output)\n",
        "    return features / features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "def encode_texts(model, processor, texts, device):\n",
        "    inputs = processor(text=texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    with torch.no_grad():\n",
        "        features = _get_text_features(model, inputs[\"input_ids\"].to(device), inputs[\"attention_mask\"].to(device))\n",
        "    return features.cpu().numpy()\n",
        "\n",
        "def load_cached_embeddings(cache_path):\n",
        "    return dict(np.load(str(cache_path)))\n",
        "\n",
        "FILLER_WORDS = frozenset({\n",
        "    \"um\", \"uh\", \"uh-huh\", \"hmm\", \"hm\", \"ah\", \"er\", \"oh\",\n",
        "    \"like\", \"you know\", \"i mean\", \"okay\", \"ok\", \"so\", \"well\",\n",
        "})\n",
        "\n",
        "def normalize_transcript(text):\n",
        "    text = text.strip().lower()\n",
        "    if not text:\n",
        "        return text\n",
        "    text = re.sub(r\"\\b(\\w+)( \\1\\b)+\", r\"\\1\", text)\n",
        "    words = text.split()\n",
        "    cleaned, skip_next = [], False\n",
        "    for i, w in enumerate(words):\n",
        "        if skip_next:\n",
        "            skip_next = False\n",
        "            continue\n",
        "        bigram = f\"{w} {words[i + 1]}\" if i + 1 < len(words) else \"\"\n",
        "        if bigram in FILLER_WORDS:\n",
        "            skip_next = True\n",
        "            continue\n",
        "        if w not in FILLER_WORDS:\n",
        "            cleaned.append(w)\n",
        "    return re.sub(r\"\\s+\", \" \", \" \".join(cleaned)).strip()\n",
        "\n",
        "def to_caption_style(text):\n",
        "    text = normalize_transcript(text)\n",
        "    if not text:\n",
        "        return text\n",
        "    return f\"an image showing {text}\" if len(text.split()) <= 2 else text\n",
        "\n",
        "def _softmax(x):\n",
        "    e = np.exp(x - x.max())\n",
        "    return e / e.sum()\n",
        "\n",
        "\n",
        "class MultimodalASR:\n",
        "    \"\"\"Whisper ASR with optional CLIP visual-context rescoring.\"\"\"\n",
        "\n",
        "    def __init__(self, whisper_model_id=DEFAULT_WHISPER_MODEL, clip_model_id=DEFAULT_CLIP_MODEL,\n",
        "                 cache_path=DEFAULT_CACHE_PATH, alpha=0.3, num_beams=5, device=None):\n",
        "        if device is None:\n",
        "            device = _pick_device()\n",
        "        self.device = device\n",
        "        self.alpha = alpha\n",
        "        self.num_beams = num_beams\n",
        "        self.whisper_processor = WhisperProcessor.from_pretrained(whisper_model_id)\n",
        "        self.whisper_model = WhisperForConditionalGeneration.from_pretrained(whisper_model_id).to(device)\n",
        "        self.whisper_model.eval()\n",
        "        self.clip_model, self.clip_processor, _ = load_clip(clip_model_id, device)\n",
        "        self.image_embeddings = {}\n",
        "        cache_path = Path(cache_path)\n",
        "        if cache_path.exists():\n",
        "            self.image_embeddings = load_cached_embeddings(cache_path)\n",
        "            print(f\"Loaded {len(self.image_embeddings)} cached image embeddings\")\n",
        "\n",
        "    def generate_nbest(self, audio_array, sr=16000):\n",
        "        input_features = self.whisper_processor(audio_array, sampling_rate=sr, return_tensors=\"pt\").input_features.to(self.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.whisper_model.generate(input_features, num_beams=self.num_beams,\n",
        "                num_return_sequences=self.num_beams, return_dict_in_generate=True, output_scores=True)\n",
        "        seq_scores = outputs.sequences_scores.cpu().numpy()\n",
        "        hypotheses, seen = [], set()\n",
        "        for i, seq in enumerate(outputs.sequences):\n",
        "            text = self.whisper_processor.decode(seq, skip_special_tokens=True).strip().lower()\n",
        "            if text not in seen:\n",
        "                seen.add(text)\n",
        "                hypotheses.append({\"text\": text, \"score\": float(seq_scores[i])})\n",
        "        hypotheses.sort(key=lambda h: h[\"score\"], reverse=True)\n",
        "        return hypotheses\n",
        "\n",
        "    def clip_similarity(self, image_embedding, texts):\n",
        "        if not texts:\n",
        "            return np.array([])\n",
        "        text_embs = encode_texts(self.clip_model, self.clip_processor, texts, self.device)\n",
        "        return np.atleast_1d((image_embedding.reshape(1, -1) @ text_embs.T).squeeze())\n",
        "\n",
        "    def rescore(self, hypotheses, image_embedding, alpha=None, caption_style=True):\n",
        "        if alpha is None:\n",
        "            alpha = self.alpha\n",
        "        if not hypotheses:\n",
        "            return hypotheses\n",
        "        texts = [h[\"text\"] for h in hypotheses]\n",
        "        clip_texts = [to_caption_style(t) if caption_style else normalize_transcript(t) for t in texts]\n",
        "        clip_scores = self.clip_similarity(image_embedding, clip_texts)\n",
        "        asr_probs = _softmax(np.array([h[\"score\"] for h in hypotheses]))\n",
        "        clip_probs = _softmax(clip_scores * 100.0)\n",
        "        fused = (1 - alpha) * asr_probs + alpha * clip_probs\n",
        "        rescored = [{\"text\": h[\"text\"], \"asr_score\": float(asr_probs[i]),\n",
        "                      \"clip_score\": float(clip_scores[i]), \"fused_score\": float(fused[i])}\n",
        "                     for i, h in enumerate(hypotheses)]\n",
        "        rescored.sort(key=lambda h: h[\"fused_score\"], reverse=True)\n",
        "        return rescored\n",
        "\n",
        "    def transcribe(self, audio_array, image_id=None, image_path=None, sr=16000, alpha=None):\n",
        "        hypotheses = self.generate_nbest(audio_array, sr)\n",
        "        if image_id is None and image_path is None:\n",
        "            return {\"transcription\": hypotheses[0][\"text\"] if hypotheses else \"\", \"hypotheses\": hypotheses, \"mode\": \"asr_only\"}\n",
        "        img_emb = self._resolve_image_embedding(image_id, image_path)\n",
        "        if img_emb is None:\n",
        "            return {\"transcription\": hypotheses[0][\"text\"] if hypotheses else \"\", \"hypotheses\": hypotheses, \"mode\": \"asr_only\",\n",
        "                    \"warning\": f\"Image not found (id={image_id}, path={image_path})\"}\n",
        "        rescored = self.rescore(hypotheses, img_emb, alpha)\n",
        "        return {\"transcription\": rescored[0][\"text\"] if rescored else \"\", \"hypotheses\": rescored,\n",
        "                \"mode\": \"multimodal\", \"alpha\": alpha if alpha is not None else self.alpha}\n",
        "\n",
        "    def _resolve_image_embedding(self, image_id, image_path):\n",
        "        if image_id and image_id in self.image_embeddings:\n",
        "            return self.image_embeddings[image_id]\n",
        "        if image_path:\n",
        "            image_path = Path(image_path)\n",
        "            if not image_path.exists():\n",
        "                return None\n",
        "            image = Image.open(image_path).convert(\"RGB\")\n",
        "            inputs = self.clip_processor(images=image, return_tensors=\"pt\")\n",
        "            with torch.no_grad():\n",
        "                emb = _get_image_features(self.clip_model, inputs[\"pixel_values\"].to(self.device))\n",
        "            return emb.cpu().numpy().squeeze()\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) `compare_asr_vs_multimodal` — per-sample comparison\n",
        "\n",
        "For each test sample, this function:\n",
        "1. Runs the pipeline **without** an image → ASR-only transcription\n",
        "2. Runs the pipeline **with** an image → multimodal transcription\n",
        "3. Computes WER for both against the ground-truth reference\n",
        "4. Records whether multimodal **improved**, **degraded**, or **unchanged** the result\n",
        "\n",
        "Each test sample should include an optional `speech_status` field (`\"dysarthria\"` or\n",
        "`\"healthy\"`) for per-group analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compare_asr_vs_multimodal(\n",
        "    pipeline: MultimodalASR,\n",
        "    test_samples: list[dict],\n",
        "    alpha: float = 0.3,\n",
        ") -> dict:\n",
        "    \"\"\"Run ASR-only and multimodal on every sample, return full analysis.\"\"\"\n",
        "    per_sample: list[dict] = []\n",
        "\n",
        "    for i, sample in enumerate(test_samples):\n",
        "        ref = sample[\"reference\"].strip().lower()\n",
        "        sr = sample.get(\"sr\", 16000)\n",
        "\n",
        "        asr_result = pipeline.transcribe(audio_array=sample[\"audio_array\"], sr=sr)\n",
        "        asr_text = asr_result[\"transcription\"]\n",
        "\n",
        "        mm_result = pipeline.transcribe(\n",
        "            audio_array=sample[\"audio_array\"],\n",
        "            image_id=sample.get(\"image_id\"),\n",
        "            image_path=sample.get(\"image_path\"),\n",
        "            sr=sr,\n",
        "            alpha=alpha,\n",
        "        )\n",
        "        mm_text = mm_result[\"transcription\"]\n",
        "\n",
        "        asr_wer = compute_wer(ref, asr_text) if ref else 0.0\n",
        "        mm_wer = compute_wer(ref, mm_text) if ref else 0.0\n",
        "\n",
        "        per_sample.append({\n",
        "            \"index\": i,\n",
        "            \"reference\": ref,\n",
        "            \"asr_hypothesis\": asr_text,\n",
        "            \"multimodal_hypothesis\": mm_text,\n",
        "            \"asr_wer\": asr_wer,\n",
        "            \"multimodal_wer\": mm_wer,\n",
        "            \"wer_delta\": mm_wer - asr_wer,\n",
        "            \"improved\": mm_wer < asr_wer,\n",
        "            \"degraded\": mm_wer > asr_wer,\n",
        "            \"unchanged\": abs(mm_wer - asr_wer) < 1e-9,\n",
        "            \"speech_status\": sample.get(\"speech_status\", \"unknown\"),\n",
        "        })\n",
        "\n",
        "        if (i + 1) % 10 == 0:\n",
        "            print(f\"  Compared {i + 1}/{len(test_samples)} samples \\u2026\")\n",
        "\n",
        "    return _aggregate(per_sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) `_aggregate` — compute summary statistics\n",
        "\n",
        "Takes the per-sample results and computes:\n",
        "- **Overall WER/CER** for ASR-only and multimodal\n",
        "- **Improvement rate** (% of samples where multimodal had lower WER)\n",
        "- **Per-group breakdown** by `speech_status` (dysarthria vs healthy)\n",
        "- **Top movers** — the 5 samples with the biggest improvement and degradation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _aggregate(per_sample: list[dict]) -> dict:\n",
        "    n = len(per_sample)\n",
        "    if n == 0:\n",
        "        return {\"error\": \"No results to analyse\"}\n",
        "\n",
        "    improved = sum(r[\"improved\"] for r in per_sample)\n",
        "    degraded = sum(r[\"degraded\"] for r in per_sample)\n",
        "    unchanged = sum(r[\"unchanged\"] for r in per_sample)\n",
        "\n",
        "    all_refs = [r[\"reference\"] for r in per_sample]\n",
        "    all_asr = [r[\"asr_hypothesis\"] for r in per_sample]\n",
        "    all_mm = [r[\"multimodal_hypothesis\"] for r in per_sample]\n",
        "\n",
        "    overall_asr_wer = compute_wer(all_refs, all_asr)\n",
        "    overall_mm_wer = compute_wer(all_refs, all_mm)\n",
        "    overall_asr_cer = compute_cer(all_refs, all_asr)\n",
        "    overall_mm_cer = compute_cer(all_refs, all_mm)\n",
        "\n",
        "    # Group by speech status\n",
        "    groups: dict[str, list[dict]] = defaultdict(list)\n",
        "    for r in per_sample:\n",
        "        groups[r[\"speech_status\"]].append(r)\n",
        "\n",
        "    group_stats: dict[str, dict] = {}\n",
        "    for status, items in groups.items():\n",
        "        g_refs = [r[\"reference\"] for r in items]\n",
        "        g_asr = [r[\"asr_hypothesis\"] for r in items]\n",
        "        g_mm = [r[\"multimodal_hypothesis\"] for r in items]\n",
        "        group_stats[status] = {\n",
        "            \"count\": len(items),\n",
        "            \"asr_wer\": compute_wer(g_refs, g_asr),\n",
        "            \"multimodal_wer\": compute_wer(g_refs, g_mm),\n",
        "            \"asr_cer\": compute_cer(g_refs, g_asr),\n",
        "            \"multimodal_cer\": compute_cer(g_refs, g_mm),\n",
        "            \"improved\": sum(r[\"improved\"] for r in items),\n",
        "            \"degraded\": sum(r[\"degraded\"] for r in items),\n",
        "        }\n",
        "\n",
        "    sorted_by_delta = sorted(per_sample, key=lambda r: r[\"wer_delta\"])\n",
        "    top_improvements = sorted_by_delta[:5]\n",
        "    top_degradations = [r for r in reversed(sorted_by_delta) if r[\"degraded\"]][:5]\n",
        "\n",
        "    return {\n",
        "        \"summary\": {\n",
        "            \"total_samples\": n,\n",
        "            \"improved\": improved,\n",
        "            \"degraded\": degraded,\n",
        "            \"unchanged\": unchanged,\n",
        "            \"improvement_rate\": improved / n,\n",
        "            \"overall_asr_wer\": overall_asr_wer,\n",
        "            \"overall_multimodal_wer\": overall_mm_wer,\n",
        "            \"overall_asr_cer\": overall_asr_cer,\n",
        "            \"overall_multimodal_cer\": overall_mm_cer,\n",
        "            \"wer_reduction\": overall_asr_wer - overall_mm_wer,\n",
        "            \"relative_improvement\": (\n",
        "                (overall_asr_wer - overall_mm_wer) / overall_asr_wer\n",
        "                if overall_asr_wer > 0 else 0.0\n",
        "            ),\n",
        "            \"success_criterion_met\": improved / n >= 0.5,\n",
        "        },\n",
        "        \"group_stats\": group_stats,\n",
        "        \"per_sample\": per_sample,\n",
        "        \"top_improvements\": top_improvements,\n",
        "        \"top_degradations\": top_degradations,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) `print_analysis` — human-readable report\n",
        "\n",
        "Prints a formatted summary including:\n",
        "- Overall improvement/degradation counts\n",
        "- WER comparison (ASR-only vs multimodal)\n",
        "- Per-group breakdown (dysarthria vs healthy)\n",
        "- Specific examples of top improvements and degradations\n",
        "- Whether the success criterion (≥50% improved) is met"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_analysis(analysis: dict):\n",
        "    \"\"\"Human-readable report.\"\"\"\n",
        "    s = analysis[\"summary\"]\n",
        "\n",
        "    print(\"=\" * 70)\n",
        "    print(\"MULTIMODAL RESCORING ANALYSIS\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    print(f\"\\n  Total samples:           {s['total_samples']}\")\n",
        "    print(f\"  Improved by multimodal:  {s['improved']} ({s['improvement_rate'] * 100:.1f}%)\")\n",
        "    print(f\"  Degraded:                {s['degraded']}\")\n",
        "    print(f\"  Unchanged:               {s['unchanged']}\")\n",
        "\n",
        "    print(f\"\\n  Overall ASR WER:         {s['overall_asr_wer'] * 100:.1f}%\")\n",
        "    print(f\"  Overall Multimodal WER:  {s['overall_multimodal_wer'] * 100:.1f}%\")\n",
        "    print(f\"  WER reduction:           {s['wer_reduction'] * 100:.1f}% abs \"\n",
        "          f\"({s['relative_improvement'] * 100:.1f}% rel)\")\n",
        "\n",
        "    criterion = \"PASSED\" if s[\"success_criterion_met\"] else \"FAILED\"\n",
        "    print(f\"\\n  Success criterion (\\u226550% helped): {criterion}\")\n",
        "\n",
        "    if analysis[\"group_stats\"]:\n",
        "        print(f\"\\n{'  Per-Group Breakdown  ':=^70}\")\n",
        "        header = f\"  {'Group':<14} {'N':>5} {'ASR WER':>9} {'MM WER':>9} {'Improved':>9} {'Degraded':>9}\"\n",
        "        print(header)\n",
        "        print(\"  \" + \"-\" * (len(header) - 2))\n",
        "        for group, d in sorted(analysis[\"group_stats\"].items()):\n",
        "            print(\n",
        "                f\"  {group:<14} {d['count']:>5} \"\n",
        "                f\"{d['asr_wer'] * 100:>8.1f}% \"\n",
        "                f\"{d['multimodal_wer'] * 100:>8.1f}% \"\n",
        "                f\"{d['improved']:>9} {d['degraded']:>9}\"\n",
        "            )\n",
        "\n",
        "    print(f\"\\n{'  Top Improvements  ':=^70}\")\n",
        "    for r in analysis.get(\"top_improvements\", [])[:5]:\n",
        "        print(f\"  [{r['index']:>4}] WER {r['asr_wer'] * 100:.0f}% \\u2192 {r['multimodal_wer'] * 100:.0f}%\"\n",
        "              f\"  ({r['speech_status']})\")\n",
        "        print(f\"        ref: {r['reference']}\")\n",
        "        print(f\"        asr: {r['asr_hypothesis']}\")\n",
        "        print(f\"        mm:  {r['multimodal_hypothesis']}\")\n",
        "\n",
        "    if analysis.get(\"top_degradations\"):\n",
        "        print(f\"\\n{'  Top Degradations  ':=^70}\")\n",
        "        for r in analysis[\"top_degradations\"][:5]:\n",
        "            print(f\"  [{r['index']:>4}] WER {r['asr_wer'] * 100:.0f}% \\u2192 {r['multimodal_wer'] * 100:.0f}%\"\n",
        "                  f\"  ({r['speech_status']})\")\n",
        "            print(f\"        ref: {r['reference']}\")\n",
        "            print(f\"        asr: {r['asr_hypothesis']}\")\n",
        "            print(f\"        mm:  {r['multimodal_hypothesis']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) `save_analysis` — persist results to JSON\n",
        "\n",
        "Saves the full analysis (including per-sample details) to a JSON file. Handles\n",
        "NumPy types that `json.dump` doesn't support natively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_analysis(analysis: dict, output_path: Path | str):\n",
        "    \"\"\"Write analysis dict to JSON (handles numpy types).\"\"\"\n",
        "    output_path = Path(output_path)\n",
        "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    class _Enc(json.JSONEncoder):\n",
        "        def default(self, o):\n",
        "            if isinstance(o, (np.integer,)):\n",
        "                return int(o)\n",
        "            if isinstance(o, (np.floating,)):\n",
        "                return float(o)\n",
        "            if isinstance(o, np.ndarray):\n",
        "                return o.tolist()\n",
        "            return super().default(o)\n",
        "\n",
        "    with open(output_path, \"w\") as f:\n",
        "        json.dump(analysis, f, indent=2, cls=_Enc)\n",
        "    print(f\"Analysis saved \\u2192 {output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Prepare test data and run analysis\n",
        "\n",
        "Like the fusion tuning notebook, this requires **paired image-audio test data**.\n",
        "Each sample needs `audio_array`, `reference`, `image_id`, and optionally\n",
        "`speech_status`.\n",
        "\n",
        "> Replace the placeholder below with your actual test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Placeholder — replace with your actual paired test data\n",
        "test_samples = []\n",
        "\n",
        "# Example:\n",
        "# import soundfile as sf\n",
        "# pairs = [\n",
        "#     (\"../audio/torgo/processed/test/sample_00000.wav\", \"img_001.png\",\n",
        "#      \"a cat sleeping on a windowsill\", \"dysarthria\"),\n",
        "# ]\n",
        "# for audio_file, image_id, reference, status in pairs:\n",
        "#     audio_array, sr = sf.read(audio_file)\n",
        "#     test_samples.append({\n",
        "#         \"audio_array\": audio_array, \"sr\": sr,\n",
        "#         \"reference\": reference, \"image_id\": image_id,\n",
        "#         \"speech_status\": status,\n",
        "#     })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if test_samples:\n",
        "    pipeline = MultimodalASR(alpha=0.3, num_beams=5)\n",
        "    analysis = compare_asr_vs_multimodal(pipeline, test_samples, alpha=0.3)\n",
        "    print_analysis(analysis)\n",
        "    save_analysis(analysis, Path(\"cache/multimodal_analysis.json\"))\n",
        "else:\n",
        "    print(\"No test samples provided. Populate test_samples in the cell above to run analysis.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Interpreting the report\n",
        "\n",
        "### Success criterion\n",
        "\n",
        "The project's success criterion is: **multimodal helps ≥ 50% of test cases**.\n",
        "\"Helps\" means the multimodal WER for that sample is strictly lower than the\n",
        "ASR-only WER.\n",
        "\n",
        "### If the criterion fails\n",
        "\n",
        "The deliverable specifies fallback strategies:\n",
        "\n",
        "1. **Improve transcript normalization** — the filler-word list and caption-wrapping\n",
        "   heuristics in `transcript_normalization.py` may need domain-specific tuning\n",
        "2. **Fine-tune CLIP** — CLIP's text encoder was trained on web captions, not\n",
        "   conversational speech. Fine-tuning on (image, therapeutic-description) pairs\n",
        "   could improve alignment\n",
        "3. **Condition on confidence** — only apply CLIP rescoring when Whisper's top-beam\n",
        "   confidence is below a threshold (i.e., the model is uncertain)\n",
        "\n",
        "### What the group breakdown reveals\n",
        "\n",
        "- If CLIP helps **dysarthric** speech more than healthy speech, multimodal rescoring\n",
        "  is successfully compensating for ASR weakness on atypical speech\n",
        "- If CLIP helps **healthy** speech more, it may be doing vocabulary disambiguation\n",
        "  rather than acoustic disambiguation — still valuable but different\n",
        "- If it **hurts** a particular group, α may need per-group tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook provides the tools to answer:\n",
        "\n",
        "| Question | Metric |\n",
        "|----------|--------|\n",
        "| Does multimodal help overall? | WER reduction (abs & relative) |\n",
        "| How often does it help? | Improvement rate (% of samples) |\n",
        "| Does it meet the success bar? | ≥ 50% improvement rate |\n",
        "| Where does it help most? | Per-group breakdown (dysarthria vs healthy) |\n",
        "| What are the failure modes? | Top degradation examples |\n",
        "\n",
        "Together with the fusion tuning results, this analysis determines whether and how\n",
        "to deploy CLIP rescoring in the final ADI/O system."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
