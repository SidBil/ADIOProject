{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP Image Embeddings — Pre-compute and Cache\n",
    "\n",
    "This notebook generates **CLIP image embeddings** for every image in the image bank\n",
    "and caches them to disk for fast reuse during multimodal ASR rescoring.\n",
    "\n",
    "## Why pre-compute?\n",
    "\n",
    "During inference, CLIP's vision encoder is the slowest part of the multimodal pipeline.\n",
    "Since our image bank is fixed (the same 20 therapeutic images are reused across sessions),\n",
    "we encode them **once** and store the resulting 512-dimensional vectors in a `.npz` file.\n",
    "At inference time, loading a cached vector is instant.\n",
    "\n",
    "## What is CLIP?\n",
    "\n",
    "**CLIP** (Contrastive Language-Image Pre-training) is a model from OpenAI that learns a\n",
    "shared embedding space for images and text. Given an image and a set of text candidates,\n",
    "CLIP can rank how well each text describes the image via **cosine similarity** between\n",
    "their embeddings. We use `openai/clip-vit-base-patch32` (ViT-B/32), which produces\n",
    "512-dimensional embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0) Imports and constants\n",
    "\n",
    "- `torch` — runs CLIP on GPU/MPS/CPU\n",
    "- `PIL.Image` — loads `.png` files into pixel arrays\n",
    "- `transformers.CLIPModel / CLIPProcessor` — the HuggingFace CLIP implementation\n",
    "- `numpy` — the cached embeddings are stored as NumPy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "\n",
    "DEFAULT_CLIP_MODEL = \"openai/clip-vit-base-patch32\"\n",
    "DEFAULT_IMAGE_DIR = Path(\"../imagegen/images\")\n",
    "DEFAULT_CACHE_DIR = Path(\"cache\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Device selection and model loading\n",
    "\n",
    "CLIP runs fastest on a GPU (`cuda`). On Apple Silicon Macs, `mps` provides GPU-like\n",
    "acceleration. We fall back to `cpu` otherwise.\n",
    "\n",
    "`load_clip` downloads the model weights from HuggingFace (cached after the first call),\n",
    "moves the model to the chosen device, and sets it to **eval mode** (disabling dropout\n",
    "and other training-only behaviours)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pick_device() -> str:\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    if torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    return \"cpu\"\n",
    "\n",
    "\n",
    "def load_clip(\n",
    "    model_name: str = DEFAULT_CLIP_MODEL, device: str | None = None\n",
    ") -> tuple[CLIPModel, CLIPProcessor, str]:\n",
    "    \"\"\"Load CLIP model and processor onto the chosen device.\"\"\"\n",
    "    if device is None:\n",
    "        device = _pick_device()\n",
    "    processor = CLIPProcessor.from_pretrained(model_name)\n",
    "    model = CLIPModel.from_pretrained(model_name).to(device)\n",
    "    model.eval()\n",
    "    return model, processor, device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Feature extraction helpers\n",
    "\n",
    "We use explicit calls to `model.vision_model()` → `model.visual_projection()` (and the\n",
    "text equivalents) rather than the higher-level `get_image_features()` shortcut. This\n",
    "ensures compatibility across different `transformers` versions.\n",
    "\n",
    "### Why L2-normalise?\n",
    "\n",
    "CLIP embeddings are compared via **cosine similarity**:\n",
    "\n",
    "$$\\text{sim}(a, b) = \\frac{a \\cdot b}{\\|a\\| \\, \\|b\\|}$$\n",
    "\n",
    "If both vectors are unit-length (L2-normalised), cosine similarity simplifies to a **dot\n",
    "product**, which is much cheaper to compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_image_features(model: CLIPModel, pixel_values: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Extract L2-normalised image embeddings.\"\"\"\n",
    "    vision_out = model.vision_model(pixel_values=pixel_values)\n",
    "    features = model.visual_projection(vision_out.pooler_output)\n",
    "    return features / features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "def _get_text_features(\n",
    "    model: CLIPModel, input_ids: torch.Tensor, attention_mask: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Extract L2-normalised text embeddings.\"\"\"\n",
    "    text_out = model.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    features = model.text_projection(text_out.pooler_output)\n",
    "    return features / features.norm(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Batch encoding functions\n",
    "\n",
    "### `encode_images`\n",
    "Iterates over every `.png` in the image directory, encodes each one through CLIP's\n",
    "vision encoder, and returns a dictionary mapping **filename → 512-d embedding**.\n",
    "\n",
    "### `encode_texts`\n",
    "Encodes a batch of text strings in one forward pass. Used later by the multimodal\n",
    "rescoring pipeline to encode ASR hypothesis candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_images(\n",
    "    model: CLIPModel,\n",
    "    processor: CLIPProcessor,\n",
    "    image_dir: Path | str,\n",
    "    device: str,\n",
    ") -> dict[str, np.ndarray]:\n",
    "    \"\"\"Encode every PNG in *image_dir* and return {filename: embedding}.\"\"\"\n",
    "    image_dir = Path(image_dir)\n",
    "    image_files = sorted(image_dir.glob(\"*.png\"))\n",
    "    if not image_files:\n",
    "        raise FileNotFoundError(f\"No .png files found in {image_dir}\")\n",
    "\n",
    "    embeddings: dict[str, np.ndarray] = {}\n",
    "    for img_path in image_files:\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        inputs = processor(images=image, return_tensors=\"pt\")\n",
    "        pixel_values = inputs[\"pixel_values\"].to(device)\n",
    "        with torch.no_grad():\n",
    "            features = _get_image_features(model, pixel_values)\n",
    "        embeddings[img_path.name] = features.cpu().numpy().squeeze()\n",
    "        print(f\"  Encoded {img_path.name}\")\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def encode_texts(\n",
    "    model: CLIPModel,\n",
    "    processor: CLIPProcessor,\n",
    "    texts: list[str],\n",
    "    device: str,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Encode a batch of texts and return L2-normalised embeddings (N, D).\"\"\"\n",
    "    inputs = processor(\n",
    "        text=texts, return_tensors=\"pt\", padding=True, truncation=True\n",
    "    )\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "    with torch.no_grad():\n",
    "        features = _get_text_features(model, input_ids, attention_mask)\n",
    "    return features.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Cache utilities\n",
    "\n",
    "Embeddings are saved with `np.savez` (compressed NumPy archive). The file maps each\n",
    "image filename (e.g. `img_001.png`) to its 512-d float32 vector.\n",
    "\n",
    "- **`cache_embeddings`** — write the dictionary to `.npz`\n",
    "- **`load_cached_embeddings`** — read it back as a plain `dict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cache_embeddings(embeddings: dict[str, np.ndarray], cache_path: Path | str):\n",
    "    \"\"\"Save image embeddings to a .npz file.\"\"\"\n",
    "    cache_path = Path(cache_path)\n",
    "    cache_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    np.savez(str(cache_path), **embeddings)\n",
    "    print(f\"Cached {len(embeddings)} embeddings \\u2192 {cache_path}\")\n",
    "\n",
    "\n",
    "def load_cached_embeddings(cache_path: Path | str) -> dict[str, np.ndarray]:\n",
    "    \"\"\"Load embeddings previously saved with *cache_embeddings*.\"\"\"\n",
    "    data = np.load(str(cache_path))\n",
    "    return dict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Run: encode all images and cache\n",
    "\n",
    "This cell loads the CLIP model, encodes all 20 images in `imagegen/images/`, saves\n",
    "the embeddings to `cache/clip_image_embeddings.npz`, and writes a metadata JSON\n",
    "summarising what was cached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, processor, device = load_clip()\n",
    "print(f\"Device: {device}  |  CLIP model: {DEFAULT_CLIP_MODEL}\")\n",
    "\n",
    "embeddings = encode_images(model, processor, DEFAULT_IMAGE_DIR, device)\n",
    "\n",
    "cache_path = DEFAULT_CACHE_DIR / \"clip_image_embeddings.npz\"\n",
    "cache_embeddings(embeddings, cache_path)\n",
    "\n",
    "metadata = {\n",
    "    \"model\": DEFAULT_CLIP_MODEL,\n",
    "    \"num_images\": len(embeddings),\n",
    "    \"image_files\": sorted(embeddings.keys()),\n",
    "    \"embedding_dim\": int(next(iter(embeddings.values())).shape[0]),\n",
    "}\n",
    "meta_path = DEFAULT_CACHE_DIR / \"clip_metadata.json\"\n",
    "with open(meta_path, \"w\") as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(f\"Metadata \\u2192 {meta_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Quick sanity check\n",
    "\n",
    "Load the cached embeddings back and verify dimensions. Then test CLIP's\n",
    "text-image alignment by comparing a few image prompts against `img_001.png`\n",
    "(a cat sleeping on a windowsill)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached = load_cached_embeddings(cache_path)\n",
    "print(f\"Loaded {len(cached)} embeddings, each {next(iter(cached.values())).shape}\")\n",
    "\n",
    "test_texts = [\n",
    "    \"a cat sleeping on a windowsill\",\n",
    "    \"a dog catching a ball in a park\",\n",
    "    \"two children building a sandcastle\",\n",
    "]\n",
    "text_embs = encode_texts(model, processor, test_texts, device)\n",
    "img_emb = cached[\"img_001.png\"].reshape(1, -1)\n",
    "\n",
    "sims = (img_emb @ text_embs.T).squeeze()\n",
    "print(\"\\nSimilarity of img_001.png to:\")\n",
    "for txt, sim in zip(test_texts, sims):\n",
    "    print(f\"  {sim:.4f}  {txt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook:\n",
    "\n",
    "1. Loaded **CLIP ViT-B/32** and selected the best available device\n",
    "2. Encoded all **20 therapeutic images** into 512-d embeddings\n",
    "3. Cached them to `cache/clip_image_embeddings.npz` for instant loading at inference time\n",
    "4. Verified that CLIP similarities align with image content\n",
    "\n",
    "The cached embeddings are used by the multimodal ASR pipeline (`multimodal_asr`) to\n",
    "rescore Whisper transcription candidates based on visual context."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
