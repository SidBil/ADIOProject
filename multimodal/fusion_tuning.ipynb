{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fusion Coefficient Tuning — Grid Search for Optimal α\n",
        "\n",
        "The multimodal rescoring pipeline combines ASR confidence with CLIP visual similarity\n",
        "using a **fusion coefficient α**:\n",
        "\n",
        "$$\\text{score} = (1 - \\alpha) \\cdot P_{\\text{ASR}} + \\alpha \\cdot P_{\\text{CLIP}}$$\n",
        "\n",
        "| α | Behaviour |\n",
        "|---|----------|\n",
        "| 0.0 | Pure ASR — CLIP is ignored entirely |\n",
        "| 0.5 | Equal weight to both signals |\n",
        "| 1.0 | Pure CLIP — ASR confidence is ignored |\n",
        "\n",
        "The optimal α depends on how informative the visual context is relative to the\n",
        "acoustic signal. This notebook finds it via **grid search** over a validation set.\n",
        "\n",
        "## Strategy\n",
        "\n",
        "Whisper beam search is the expensive part. We run it **once** per sample to\n",
        "pre-compute all *n*-best hypotheses, then sweep α values cheaply by just\n",
        "re-running the fusion arithmetic."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a980f84",
      "metadata": {},
      "source": [
        "## 0) Imports and prerequisite code\n",
        "\n",
        "This notebook is **self-contained** — it includes the full `MultimodalASR` pipeline\n",
        "(CLIP helpers, transcript normalization, and the pipeline class) inline so it can\n",
        "run independently. We also import `jiwer` for computing Word Error Rate at each α."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import soundfile as sf\n",
        "from PIL import Image\n",
        "from jiwer import wer as compute_wer\n",
        "from transformers import (\n",
        "    WhisperProcessor,\n",
        "    WhisperForConditionalGeneration,\n",
        "    CLIPModel,\n",
        "    CLIPProcessor,\n",
        ")\n",
        "\n",
        "DEFAULT_ALPHA_RANGE = np.round(np.arange(0.0, 1.05, 0.05), 2)\n",
        "\n",
        "# ── Prerequisite code (defined in clip_embeddings & multimodal_asr notebooks) ──\n",
        "# Included inline so this notebook is self-contained and runnable on its own.\n",
        "\n",
        "DEFAULT_CLIP_MODEL = \"openai/clip-vit-base-patch32\"\n",
        "DEFAULT_WHISPER_MODEL = \"openai/whisper-small\"\n",
        "DEFAULT_CACHE_PATH = Path(\"cache/clip_image_embeddings.npz\")\n",
        "\n",
        "def _pick_device():\n",
        "    if torch.cuda.is_available():\n",
        "        return \"cuda\"\n",
        "    if torch.backends.mps.is_available():\n",
        "        return \"mps\"\n",
        "    return \"cpu\"\n",
        "\n",
        "def load_clip(model_name=DEFAULT_CLIP_MODEL, device=None):\n",
        "    if device is None:\n",
        "        device = _pick_device()\n",
        "    processor = CLIPProcessor.from_pretrained(model_name)\n",
        "    model = CLIPModel.from_pretrained(model_name).to(device)\n",
        "    model.eval()\n",
        "    return model, processor, device\n",
        "\n",
        "def _get_image_features(model, pixel_values):\n",
        "    vision_out = model.vision_model(pixel_values=pixel_values)\n",
        "    features = model.visual_projection(vision_out.pooler_output)\n",
        "    return features / features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "def _get_text_features(model, input_ids, attention_mask):\n",
        "    text_out = model.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    features = model.text_projection(text_out.pooler_output)\n",
        "    return features / features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "def encode_texts(model, processor, texts, device):\n",
        "    inputs = processor(text=texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    with torch.no_grad():\n",
        "        features = _get_text_features(model, inputs[\"input_ids\"].to(device), inputs[\"attention_mask\"].to(device))\n",
        "    return features.cpu().numpy()\n",
        "\n",
        "def load_cached_embeddings(cache_path):\n",
        "    return dict(np.load(str(cache_path)))\n",
        "\n",
        "FILLER_WORDS = frozenset({\n",
        "    \"um\", \"uh\", \"uh-huh\", \"hmm\", \"hm\", \"ah\", \"er\", \"oh\",\n",
        "    \"like\", \"you know\", \"i mean\", \"okay\", \"ok\", \"so\", \"well\",\n",
        "})\n",
        "\n",
        "def normalize_transcript(text):\n",
        "    text = text.strip().lower()\n",
        "    if not text:\n",
        "        return text\n",
        "    text = re.sub(r\"\\b(\\w+)( \\1\\b)+\", r\"\\1\", text)\n",
        "    words = text.split()\n",
        "    cleaned, skip_next = [], False\n",
        "    for i, w in enumerate(words):\n",
        "        if skip_next:\n",
        "            skip_next = False\n",
        "            continue\n",
        "        bigram = f\"{w} {words[i + 1]}\" if i + 1 < len(words) else \"\"\n",
        "        if bigram in FILLER_WORDS:\n",
        "            skip_next = True\n",
        "            continue\n",
        "        if w not in FILLER_WORDS:\n",
        "            cleaned.append(w)\n",
        "    return re.sub(r\"\\s+\", \" \", \" \".join(cleaned)).strip()\n",
        "\n",
        "def to_caption_style(text):\n",
        "    text = normalize_transcript(text)\n",
        "    if not text:\n",
        "        return text\n",
        "    return f\"an image showing {text}\" if len(text.split()) <= 2 else text\n",
        "\n",
        "def _softmax(x):\n",
        "    e = np.exp(x - x.max())\n",
        "    return e / e.sum()\n",
        "\n",
        "\n",
        "class MultimodalASR:\n",
        "    \"\"\"Whisper ASR with optional CLIP visual-context rescoring.\"\"\"\n",
        "\n",
        "    def __init__(self, whisper_model_id=DEFAULT_WHISPER_MODEL, clip_model_id=DEFAULT_CLIP_MODEL,\n",
        "                 cache_path=DEFAULT_CACHE_PATH, alpha=0.3, num_beams=5, device=None):\n",
        "        if device is None:\n",
        "            device = _pick_device()\n",
        "        self.device = device\n",
        "        self.alpha = alpha\n",
        "        self.num_beams = num_beams\n",
        "        self.whisper_processor = WhisperProcessor.from_pretrained(whisper_model_id)\n",
        "        self.whisper_model = WhisperForConditionalGeneration.from_pretrained(whisper_model_id).to(device)\n",
        "        self.whisper_model.eval()\n",
        "        self.clip_model, self.clip_processor, _ = load_clip(clip_model_id, device)\n",
        "        self.image_embeddings = {}\n",
        "        cache_path = Path(cache_path)\n",
        "        if cache_path.exists():\n",
        "            self.image_embeddings = load_cached_embeddings(cache_path)\n",
        "            print(f\"Loaded {len(self.image_embeddings)} cached image embeddings\")\n",
        "\n",
        "    def generate_nbest(self, audio_array, sr=16000):\n",
        "        input_features = self.whisper_processor(audio_array, sampling_rate=sr, return_tensors=\"pt\").input_features.to(self.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.whisper_model.generate(input_features, num_beams=self.num_beams,\n",
        "                num_return_sequences=self.num_beams, return_dict_in_generate=True, output_scores=True)\n",
        "        seq_scores = outputs.sequences_scores.cpu().numpy()\n",
        "        hypotheses, seen = [], set()\n",
        "        for i, seq in enumerate(outputs.sequences):\n",
        "            text = self.whisper_processor.decode(seq, skip_special_tokens=True).strip().lower()\n",
        "            if text not in seen:\n",
        "                seen.add(text)\n",
        "                hypotheses.append({\"text\": text, \"score\": float(seq_scores[i])})\n",
        "        hypotheses.sort(key=lambda h: h[\"score\"], reverse=True)\n",
        "        return hypotheses\n",
        "\n",
        "    def clip_similarity(self, image_embedding, texts):\n",
        "        if not texts:\n",
        "            return np.array([])\n",
        "        text_embs = encode_texts(self.clip_model, self.clip_processor, texts, self.device)\n",
        "        return np.atleast_1d((image_embedding.reshape(1, -1) @ text_embs.T).squeeze())\n",
        "\n",
        "    def rescore(self, hypotheses, image_embedding, alpha=None, caption_style=True):\n",
        "        if alpha is None:\n",
        "            alpha = self.alpha\n",
        "        if not hypotheses:\n",
        "            return hypotheses\n",
        "        texts = [h[\"text\"] for h in hypotheses]\n",
        "        clip_texts = [to_caption_style(t) if caption_style else normalize_transcript(t) for t in texts]\n",
        "        clip_scores = self.clip_similarity(image_embedding, clip_texts)\n",
        "        asr_probs = _softmax(np.array([h[\"score\"] for h in hypotheses]))\n",
        "        clip_probs = _softmax(clip_scores * 100.0)\n",
        "        fused = (1 - alpha) * asr_probs + alpha * clip_probs\n",
        "        rescored = [{\"text\": h[\"text\"], \"asr_score\": float(asr_probs[i]),\n",
        "                      \"clip_score\": float(clip_scores[i]), \"fused_score\": float(fused[i])}\n",
        "                     for i, h in enumerate(hypotheses)]\n",
        "        rescored.sort(key=lambda h: h[\"fused_score\"], reverse=True)\n",
        "        return rescored\n",
        "\n",
        "    def transcribe(self, audio_array, image_id=None, image_path=None, sr=16000, alpha=None):\n",
        "        hypotheses = self.generate_nbest(audio_array, sr)\n",
        "        if image_id is None and image_path is None:\n",
        "            return {\"transcription\": hypotheses[0][\"text\"] if hypotheses else \"\", \"hypotheses\": hypotheses, \"mode\": \"asr_only\"}\n",
        "        img_emb = self._resolve_image_embedding(image_id, image_path)\n",
        "        if img_emb is None:\n",
        "            return {\"transcription\": hypotheses[0][\"text\"] if hypotheses else \"\", \"hypotheses\": hypotheses, \"mode\": \"asr_only\",\n",
        "                    \"warning\": f\"Image not found (id={image_id}, path={image_path})\"}\n",
        "        rescored = self.rescore(hypotheses, img_emb, alpha)\n",
        "        return {\"transcription\": rescored[0][\"text\"] if rescored else \"\", \"hypotheses\": rescored,\n",
        "                \"mode\": \"multimodal\", \"alpha\": alpha if alpha is not None else self.alpha}\n",
        "\n",
        "    def _resolve_image_embedding(self, image_id, image_path):\n",
        "        if image_id and image_id in self.image_embeddings:\n",
        "            return self.image_embeddings[image_id]\n",
        "        if image_path:\n",
        "            image_path = Path(image_path)\n",
        "            if not image_path.exists():\n",
        "                return None\n",
        "            image = Image.open(image_path).convert(\"RGB\")\n",
        "            inputs = self.clip_processor(images=image, return_tensors=\"pt\")\n",
        "            with torch.no_grad():\n",
        "                emb = _get_image_features(self.clip_model, inputs[\"pixel_values\"].to(self.device))\n",
        "            return emb.cpu().numpy().squeeze()\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) `tune_alpha` — the grid search function\n",
        "\n",
        "### Input format\n",
        "\n",
        "Each test sample is a dictionary with:\n",
        "\n",
        "| Key | Type | Description |\n",
        "|-----|------|------------|\n",
        "| `audio_array` | `np.ndarray` | Raw waveform |\n",
        "| `sr` | `int` | Sampling rate (default 16 000) |\n",
        "| `reference` | `str` | Ground-truth transcription |\n",
        "| `image_id` | `str` | Cached embedding key, e.g. `\"img_001.png\"` |\n",
        "\n",
        "### Algorithm\n",
        "\n",
        "1. **Pre-compute** (one-time cost): for each sample, run Whisper beam search to\n",
        "   get *n*-best hypotheses, and look up the image embedding from cache.\n",
        "2. **Sweep** α from 0.0 to 1.0 in steps of 0.05 (21 values). For each α:\n",
        "   - Re-run the fusion formula on all pre-computed hypotheses\n",
        "   - Pick the top hypothesis for each sample\n",
        "   - Compute corpus-level WER against references\n",
        "3. **Select** the α with the lowest WER."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tune_alpha(\n",
        "    pipeline: MultimodalASR,\n",
        "    test_samples: list[dict],\n",
        "    alpha_range: np.ndarray = DEFAULT_ALPHA_RANGE,\n",
        ") -> dict:\n",
        "    \"\"\"Grid-search alpha on paired image-audio test samples.\"\"\"\n",
        "\n",
        "    # Step 1: pre-compute n-best hypotheses + image embeddings\n",
        "    print(f\"Pre-computing n-best hypotheses for {len(test_samples)} samples \\u2026\")\n",
        "    precomputed: list[dict] = []\n",
        "\n",
        "    for i, sample in enumerate(test_samples):\n",
        "        hyps = pipeline.generate_nbest(\n",
        "            sample[\"audio_array\"], sample.get(\"sr\", 16000)\n",
        "        )\n",
        "        img_emb = pipeline._resolve_image_embedding(\n",
        "            sample.get(\"image_id\"), sample.get(\"image_path\")\n",
        "        )\n",
        "        precomputed.append({\n",
        "            \"hypotheses\": hyps,\n",
        "            \"image_embedding\": img_emb,\n",
        "            \"reference\": sample[\"reference\"].strip().lower(),\n",
        "        })\n",
        "        if (i + 1) % 10 == 0:\n",
        "            print(f\"  {i + 1}/{len(test_samples)} done\")\n",
        "\n",
        "    # Step 2: sweep alpha\n",
        "    print(f\"\\nSweeping {len(alpha_range)} alpha values \\u2026\")\n",
        "    grid_results: list[dict] = []\n",
        "\n",
        "    for alpha in alpha_range:\n",
        "        alpha = float(alpha)\n",
        "        refs, hyps_texts = [], []\n",
        "\n",
        "        for item in precomputed:\n",
        "            ref = item[\"reference\"]\n",
        "            if item[\"image_embedding\"] is not None:\n",
        "                rescored = pipeline.rescore(\n",
        "                    item[\"hypotheses\"], item[\"image_embedding\"], alpha\n",
        "                )\n",
        "                best = rescored[0][\"text\"] if rescored else \"\"\n",
        "            else:\n",
        "                best = (\n",
        "                    item[\"hypotheses\"][0][\"text\"] if item[\"hypotheses\"] else \"\"\n",
        "                )\n",
        "            refs.append(ref)\n",
        "            hyps_texts.append(best)\n",
        "\n",
        "        alpha_wer = compute_wer(refs, hyps_texts)\n",
        "        grid_results.append({\"alpha\": alpha, \"wer\": alpha_wer})\n",
        "        print(f\"  \\u03b1 = {alpha:.2f}   WER = {alpha_wer * 100:.1f}%\")\n",
        "\n",
        "    best = min(grid_results, key=lambda r: r[\"wer\"])\n",
        "    baseline_wer = next(r[\"wer\"] for r in grid_results if r[\"alpha\"] == 0.0)\n",
        "\n",
        "    summary = {\n",
        "        \"grid_results\": grid_results,\n",
        "        \"best_alpha\": best[\"alpha\"],\n",
        "        \"best_wer\": best[\"wer\"],\n",
        "        \"baseline_wer\": baseline_wer,\n",
        "        \"wer_reduction\": baseline_wer - best[\"wer\"],\n",
        "        \"num_samples\": len(test_samples),\n",
        "    }\n",
        "\n",
        "    print(f\"\\n{'\\u2500' * 50}\")\n",
        "    print(f\"Baseline WER (\\u03b1=0):  {baseline_wer * 100:.1f}%\")\n",
        "    print(f\"Best WER:            {best['wer'] * 100:.1f}%  (\\u03b1 = {best['alpha']:.2f})\")\n",
        "    print(f\"Absolute reduction:  {(baseline_wer - best['wer']) * 100:.1f}%\")\n",
        "\n",
        "    return summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Prepare test data\n",
        "\n",
        "To run the grid search, you need **paired image-audio test data** — audio samples\n",
        "where you know both the ground-truth transcription and which image was being described.\n",
        "\n",
        "Below is a template that you can adapt to your test set. Replace the placeholder\n",
        "with your actual paired samples.\n",
        "\n",
        "```python\n",
        "import soundfile as sf\n",
        "\n",
        "test_samples = []\n",
        "for audio_file, image_id, reference in your_test_pairs:\n",
        "    audio_array, sr = sf.read(audio_file)\n",
        "    test_samples.append({\n",
        "        \"audio_array\": audio_array,\n",
        "        \"sr\": sr,\n",
        "        \"reference\": reference,\n",
        "        \"image_id\": image_id,\n",
        "    })\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Placeholder — replace with your actual paired test data\n",
        "test_samples = []\n",
        "\n",
        "# Example: uncomment and adapt\n",
        "# import soundfile as sf\n",
        "# pairs = [\n",
        "#     (\"../audio/torgo/processed/test/sample_00000.wav\", \"img_001.png\", \"a cat sleeping on a windowsill\"),\n",
        "#     (\"../audio/torgo/processed/test/sample_00001.wav\", \"img_002.png\", \"a dog catching a ball in a park\"),\n",
        "# ]\n",
        "# for audio_file, image_id, reference in pairs:\n",
        "#     audio_array, sr = sf.read(audio_file)\n",
        "#     test_samples.append({\n",
        "#         \"audio_array\": audio_array, \"sr\": sr,\n",
        "#         \"reference\": reference, \"image_id\": image_id,\n",
        "#     })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Run the grid search\n",
        "\n",
        "Initialise the pipeline and run `tune_alpha`. The output shows WER at each α\n",
        "and highlights the optimum.\n",
        "\n",
        "> **Note**: This cell requires `test_samples` to be populated above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if test_samples:\n",
        "    pipeline = MultimodalASR(alpha=0.0, num_beams=5)\n",
        "    results = tune_alpha(pipeline, test_samples)\n",
        "\n",
        "    # Save results\n",
        "    output_path = Path(\"cache/fusion_tuning_results.json\")\n",
        "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with open(output_path, \"w\") as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "    print(f\"\\nResults saved \\u2192 {output_path}\")\n",
        "else:\n",
        "    print(\"No test samples provided. Populate test_samples in the cell above to run tuning.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Interpreting results\n",
        "\n",
        "After running the grid search, you'll see output like:\n",
        "\n",
        "```\n",
        "α = 0.00   WER = 61.8%     ← baseline (pure ASR)\n",
        "α = 0.05   WER = 60.2%\n",
        "α = 0.10   WER = 58.5%\n",
        "  ...\n",
        "α = 0.25   WER = 55.1%     ← best\n",
        "  ...\n",
        "α = 1.00   WER = 89.3%     ← pure CLIP (too aggressive)\n",
        "```\n",
        "\n",
        "**What to look for:**\n",
        "- The best α is usually in the **0.1–0.4 range** — visual context helps but\n",
        "  shouldn't override strong ASR confidence.\n",
        "- If the best α is 0.0, CLIP isn't helping. Consider:\n",
        "  - Is the image-text alignment strong enough?\n",
        "  - Does the transcript normalization need tuning?\n",
        "  - Would fine-tuning CLIP on your domain help?\n",
        "- Very high optimal α (>0.5) suggests the ASR model is struggling badly and\n",
        "  visual context is doing most of the disambiguation work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook:\n",
        "\n",
        "1. Pre-computes Whisper beam hypotheses once (expensive)\n",
        "2. Sweeps 21 α values from 0.0 to 1.0 (cheap)\n",
        "3. Selects the α that minimises corpus-level WER\n",
        "4. Saves the full grid results to JSON\n",
        "\n",
        "Use the optimal α when deploying the `MultimodalASR` pipeline:\n",
        "\n",
        "```python\n",
        "pipeline = MultimodalASR(alpha=results[\"best_alpha\"])\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
