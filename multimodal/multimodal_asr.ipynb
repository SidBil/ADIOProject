{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multimodal ASR — Whisper + CLIP Rescoring Pipeline\n",
        "\n",
        "This notebook implements the **core multimodal transcription pipeline**:\n",
        "\n",
        "1. **Whisper beam search** generates *n*-best transcription hypotheses with log-prob scores\n",
        "2. **CLIP** computes cosine similarity between the visual context (image) and each text hypothesis\n",
        "3. **Fusion** combines both signals: `score = (1-α)·ASR_prob + α·CLIP_prob`\n",
        "4. **Re-rank** and return the best hypothesis\n",
        "\n",
        "## Why multimodal rescoring?\n",
        "\n",
        "In the ADI/O therapeutic setting, a child is shown an image and asked to describe it.\n",
        "When the speech is dysarthric, Whisper's 1-best transcription may be wrong — but the\n",
        "correct answer is often *somewhere* in the beam. CLIP can identify which hypothesis\n",
        "best matches the image content, resolving ambiguities that pure audio cannot.\n",
        "\n",
        "For example, if the image shows a **bear**, Whisper might confuse \"bear\" with \"bare\".\n",
        "CLIP's image-text similarity strongly favours \"bear\" when a bear is visible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0) Imports and configuration\n",
        "\n",
        "This notebook is **self-contained** — all CLIP helper functions, transcript normalization,\n",
        "and the main pipeline class are defined inline.\n",
        "\n",
        "If you haven't already, run the `clip_embeddings` notebook first to generate the\n",
        "cached image embeddings in `cache/`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79d18b20",
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import soundfile as sf\n",
        "from PIL import Image\n",
        "from transformers import (\n",
        "    WhisperProcessor,\n",
        "    WhisperForConditionalGeneration,\n",
        "    CLIPModel,\n",
        "    CLIPProcessor,\n",
        ")\n",
        "\n",
        "DEFAULT_CLIP_MODEL = \"openai/clip-vit-base-patch32\"\n",
        "DEFAULT_WHISPER_MODEL = \"openai/whisper-small\"\n",
        "DEFAULT_CACHE_PATH = Path(\"cache/clip_image_embeddings.npz\")\n",
        "\n",
        "# ── CLIP helpers (from clip_embeddings notebook) ─────────────────────\n",
        "\n",
        "def _pick_device() -> str:\n",
        "    if torch.cuda.is_available():\n",
        "        return \"cuda\"\n",
        "    if torch.backends.mps.is_available():\n",
        "        return \"mps\"\n",
        "    return \"cpu\"\n",
        "\n",
        "def load_clip(model_name=DEFAULT_CLIP_MODEL, device=None):\n",
        "    if device is None:\n",
        "        device = _pick_device()\n",
        "    processor = CLIPProcessor.from_pretrained(model_name)\n",
        "    model = CLIPModel.from_pretrained(model_name).to(device)\n",
        "    model.eval()\n",
        "    return model, processor, device\n",
        "\n",
        "def _get_image_features(model, pixel_values):\n",
        "    vision_out = model.vision_model(pixel_values=pixel_values)\n",
        "    features = model.visual_projection(vision_out.pooler_output)\n",
        "    return features / features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "def _get_text_features(model, input_ids, attention_mask):\n",
        "    text_out = model.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    features = model.text_projection(text_out.pooler_output)\n",
        "    return features / features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "def encode_texts(model, processor, texts, device):\n",
        "    inputs = processor(text=texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    input_ids = inputs[\"input_ids\"].to(device)\n",
        "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
        "    with torch.no_grad():\n",
        "        features = _get_text_features(model, input_ids, attention_mask)\n",
        "    return features.cpu().numpy()\n",
        "\n",
        "def load_cached_embeddings(cache_path):\n",
        "    data = np.load(str(cache_path))\n",
        "    return dict(data)\n",
        "\n",
        "# ── Transcript normalization (from transcript_normalization notebook) ─\n",
        "\n",
        "FILLER_WORDS = frozenset({\n",
        "    \"um\", \"uh\", \"uh-huh\", \"hmm\", \"hm\", \"ah\", \"er\", \"oh\",\n",
        "    \"like\", \"you know\", \"i mean\", \"okay\", \"ok\", \"so\", \"well\",\n",
        "})\n",
        "\n",
        "def normalize_transcript(text):\n",
        "    text = text.strip().lower()\n",
        "    if not text:\n",
        "        return text\n",
        "    text = re.sub(r\"\\b(\\w+)( \\1\\b)+\", r\"\\1\", text)\n",
        "    words = text.split()\n",
        "    cleaned = []\n",
        "    skip_next = False\n",
        "    for i, w in enumerate(words):\n",
        "        if skip_next:\n",
        "            skip_next = False\n",
        "            continue\n",
        "        bigram = f\"{w} {words[i + 1]}\" if i + 1 < len(words) else \"\"\n",
        "        if bigram in FILLER_WORDS:\n",
        "            skip_next = True\n",
        "            continue\n",
        "        if w not in FILLER_WORDS:\n",
        "            cleaned.append(w)\n",
        "    return re.sub(r\"\\s+\", \" \", \" \".join(cleaned)).strip()\n",
        "\n",
        "def to_caption_style(text):\n",
        "    text = normalize_transcript(text)\n",
        "    if not text:\n",
        "        return text\n",
        "    if len(text.split()) <= 2:\n",
        "        return f\"an image showing {text}\"\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Softmax helper\n",
        "\n",
        "Both ASR log-probabilities and CLIP cosine similarities need to be converted into\n",
        "probability distributions before fusion. We use the numerically stable softmax:\n",
        "\n",
        "$$\\text{softmax}(x_i) = \\frac{e^{x_i - \\max(x)}}{\\sum_j e^{x_j - \\max(x)}}$$\n",
        "\n",
        "Subtracting the max prevents overflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _softmax(x: np.ndarray) -> np.ndarray:\n",
        "    e = np.exp(x - x.max())\n",
        "    return e / e.sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) `MultimodalASR` class\n",
        "\n",
        "This is the main pipeline class. It holds both models (Whisper + CLIP) and\n",
        "provides three key methods:\n",
        "\n",
        "### `generate_nbest(audio_array, sr)`\n",
        "Runs Whisper with **beam search** (`num_beams=5` by default) and returns up to 5\n",
        "unique hypotheses, each with a length-normalised log-probability score.\n",
        "Duplicate beams are filtered out.\n",
        "\n",
        "### `rescore(hypotheses, image_embedding, alpha)`\n",
        "The fusion step:\n",
        "1. Normalise ASR log-probs → probabilities via softmax\n",
        "2. Encode each hypothesis text through CLIP's text encoder\n",
        "3. Compute cosine similarity with the image embedding\n",
        "4. Scale CLIP similarities by temperature τ≈100 (CLIP's learned logit scale), then softmax\n",
        "5. Fuse: `final = (1-α)·asr_probs + α·clip_probs`\n",
        "6. Re-sort by fused score\n",
        "\n",
        "### `transcribe(audio_array, image_id=, image_path=, sr=, alpha=)`\n",
        "The top-level entry point that chains `generate_nbest` → `rescore` → return best.\n",
        "Pass an `image_id` (for cached embeddings) or `image_path` (computed on-the-fly).\n",
        "Omit both for ASR-only transcription."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultimodalASR:\n",
        "    \"\"\"Whisper ASR with optional CLIP visual-context rescoring.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        whisper_model_id: str = DEFAULT_WHISPER_MODEL,\n",
        "        clip_model_id: str = DEFAULT_CLIP_MODEL,\n",
        "        cache_path: Path | str = DEFAULT_CACHE_PATH,\n",
        "        alpha: float = 0.3,\n",
        "        num_beams: int = 5,\n",
        "        device: str | None = None,\n",
        "    ):\n",
        "        if device is None:\n",
        "            device = (\n",
        "                \"cuda\" if torch.cuda.is_available()\n",
        "                else \"mps\" if torch.backends.mps.is_available()\n",
        "                else \"cpu\"\n",
        "            )\n",
        "        self.device = device\n",
        "        self.alpha = alpha\n",
        "        self.num_beams = num_beams\n",
        "\n",
        "        # Whisper\n",
        "        self.whisper_processor = WhisperProcessor.from_pretrained(whisper_model_id)\n",
        "        self.whisper_model = WhisperForConditionalGeneration.from_pretrained(\n",
        "            whisper_model_id\n",
        "        ).to(device)\n",
        "        self.whisper_model.eval()\n",
        "\n",
        "        # CLIP\n",
        "        self.clip_model, self.clip_processor, _ = load_clip(clip_model_id, device)\n",
        "\n",
        "        # Pre-computed image embeddings\n",
        "        self.image_embeddings: dict[str, np.ndarray] = {}\n",
        "        cache_path = Path(cache_path)\n",
        "        if cache_path.exists():\n",
        "            self.image_embeddings = load_cached_embeddings(cache_path)\n",
        "            print(f\"Loaded {len(self.image_embeddings)} cached image embeddings\")\n",
        "\n",
        "    # ── n-best generation ────────────────────────────────────────────\n",
        "\n",
        "    def generate_nbest(\n",
        "        self, audio_array: np.ndarray, sr: int = 16000\n",
        "    ) -> list[dict]:\n",
        "        \"\"\"Return n-best Whisper hypotheses sorted by score (descending).\"\"\"\n",
        "        input_features = self.whisper_processor(\n",
        "            audio_array, sampling_rate=sr, return_tensors=\"pt\"\n",
        "        ).input_features.to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.whisper_model.generate(\n",
        "                input_features,\n",
        "                num_beams=self.num_beams,\n",
        "                num_return_sequences=self.num_beams,\n",
        "                return_dict_in_generate=True,\n",
        "                output_scores=True,\n",
        "            )\n",
        "\n",
        "        sequences = outputs.sequences\n",
        "        seq_scores = outputs.sequences_scores.cpu().numpy()\n",
        "\n",
        "        hypotheses: list[dict] = []\n",
        "        seen: set[str] = set()\n",
        "        for i, seq in enumerate(sequences):\n",
        "            text = self.whisper_processor.decode(\n",
        "                seq, skip_special_tokens=True\n",
        "            ).strip().lower()\n",
        "            if text in seen:\n",
        "                continue\n",
        "            seen.add(text)\n",
        "            hypotheses.append({\"text\": text, \"score\": float(seq_scores[i])})\n",
        "\n",
        "        hypotheses.sort(key=lambda h: h[\"score\"], reverse=True)\n",
        "        return hypotheses\n",
        "\n",
        "    # ── CLIP similarity ──────────────────────────────────────────────\n",
        "\n",
        "    def clip_similarity(\n",
        "        self, image_embedding: np.ndarray, texts: list[str]\n",
        "    ) -> np.ndarray:\n",
        "        \"\"\"Cosine similarities between one image embedding and N texts.\"\"\"\n",
        "        if not texts:\n",
        "            return np.array([])\n",
        "        text_embeddings = encode_texts(\n",
        "            self.clip_model, self.clip_processor, texts, self.device\n",
        "        )\n",
        "        sims = (image_embedding.reshape(1, -1) @ text_embeddings.T).squeeze()\n",
        "        return np.atleast_1d(sims)\n",
        "\n",
        "    # ── rescoring ────────────────────────────────────────────────────\n",
        "\n",
        "    def rescore(\n",
        "        self,\n",
        "        hypotheses: list[dict],\n",
        "        image_embedding: np.ndarray,\n",
        "        alpha: float | None = None,\n",
        "        caption_style: bool = True,\n",
        "    ) -> list[dict]:\n",
        "        \"\"\"Fuse ASR log-probs with CLIP cosine similarity.\"\"\"\n",
        "        if alpha is None:\n",
        "            alpha = self.alpha\n",
        "        if not hypotheses:\n",
        "            return hypotheses\n",
        "\n",
        "        texts = [h[\"text\"] for h in hypotheses]\n",
        "        clip_texts = [\n",
        "            to_caption_style(t) if caption_style else normalize_transcript(t)\n",
        "            for t in texts\n",
        "        ]\n",
        "\n",
        "        clip_scores = self.clip_similarity(image_embedding, clip_texts)\n",
        "\n",
        "        asr_logits = np.array([h[\"score\"] for h in hypotheses])\n",
        "        asr_probs = _softmax(asr_logits)\n",
        "\n",
        "        clip_probs = _softmax(clip_scores * 100.0)\n",
        "\n",
        "        fused = (1 - alpha) * asr_probs + alpha * clip_probs\n",
        "\n",
        "        rescored = [\n",
        "            {\n",
        "                \"text\": h[\"text\"],\n",
        "                \"asr_score\": float(asr_probs[i]),\n",
        "                \"clip_score\": float(clip_scores[i]),\n",
        "                \"fused_score\": float(fused[i]),\n",
        "            }\n",
        "            for i, h in enumerate(hypotheses)\n",
        "        ]\n",
        "        rescored.sort(key=lambda h: h[\"fused_score\"], reverse=True)\n",
        "        return rescored\n",
        "\n",
        "    # ── main entry point ─────────────────────────────────────────────\n",
        "\n",
        "    def transcribe(\n",
        "        self,\n",
        "        audio_array: np.ndarray,\n",
        "        image_id: str | None = None,\n",
        "        image_path: str | Path | None = None,\n",
        "        sr: int = 16000,\n",
        "        alpha: float | None = None,\n",
        "    ) -> dict:\n",
        "        \"\"\"Full transcription pipeline.\"\"\"\n",
        "        hypotheses = self.generate_nbest(audio_array, sr)\n",
        "\n",
        "        if image_id is None and image_path is None:\n",
        "            return {\n",
        "                \"transcription\": hypotheses[0][\"text\"] if hypotheses else \"\",\n",
        "                \"hypotheses\": hypotheses,\n",
        "                \"mode\": \"asr_only\",\n",
        "            }\n",
        "\n",
        "        img_emb = self._resolve_image_embedding(image_id, image_path)\n",
        "        if img_emb is None:\n",
        "            return {\n",
        "                \"transcription\": hypotheses[0][\"text\"] if hypotheses else \"\",\n",
        "                \"hypotheses\": hypotheses,\n",
        "                \"mode\": \"asr_only\",\n",
        "                \"warning\": f\"Image not found (id={image_id}, path={image_path})\",\n",
        "            }\n",
        "\n",
        "        rescored = self.rescore(hypotheses, img_emb, alpha)\n",
        "        return {\n",
        "            \"transcription\": rescored[0][\"text\"] if rescored else \"\",\n",
        "            \"hypotheses\": rescored,\n",
        "            \"mode\": \"multimodal\",\n",
        "            \"alpha\": alpha if alpha is not None else self.alpha,\n",
        "        }\n",
        "\n",
        "    # ── helpers ───────────────────────────────────────────────────────\n",
        "\n",
        "    def _resolve_image_embedding(\n",
        "        self, image_id: str | None, image_path: str | Path | None\n",
        "    ) -> np.ndarray | None:\n",
        "        if image_id and image_id in self.image_embeddings:\n",
        "            return self.image_embeddings[image_id]\n",
        "        if image_path:\n",
        "            image_path = Path(image_path)\n",
        "            if not image_path.exists():\n",
        "                return None\n",
        "            image = Image.open(image_path).convert(\"RGB\")\n",
        "            inputs = self.clip_processor(images=image, return_tensors=\"pt\")\n",
        "            pixel_values = inputs[\"pixel_values\"].to(self.device)\n",
        "            with torch.no_grad():\n",
        "                emb = _get_image_features(self.clip_model, pixel_values)\n",
        "            return emb.cpu().numpy().squeeze()\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Initialise the pipeline\n",
        "\n",
        "This loads **both** Whisper Small and CLIP ViT-B/32, plus the cached image embeddings.\n",
        "It takes a moment the first time (model weights are downloaded and cached by HuggingFace)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline = MultimodalASR(\n",
        "    whisper_model_id=DEFAULT_WHISPER_MODEL,\n",
        "    alpha=0.3,\n",
        "    num_beams=5,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Demo: ASR-only vs multimodal transcription\n",
        "\n",
        "Pick a test audio file and an image. We run the pipeline twice — once without an\n",
        "image (pure ASR) and once with the image (multimodal rescoring) — and compare the\n",
        "hypotheses and their scores.\n",
        "\n",
        "If CLIP helps, the multimodal ranking will differ from the ASR-only ranking, and the\n",
        "top hypothesis will be more semantically aligned with the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "audio_path = \"../audio/torgo/processed/test/sample_00007.wav\"\n",
        "image_id = \"img_001.png\"\n",
        "\n",
        "audio_array, sr = sf.read(audio_path)\n",
        "print(f\"Audio: {audio_path}  ({len(audio_array)/sr:.1f}s @ {sr} Hz)\")\n",
        "\n",
        "# ASR-only\n",
        "asr_result = pipeline.transcribe(audio_array, sr=sr)\n",
        "print(f\"\\n--- ASR-only (mode: {asr_result['mode']}) ---\")\n",
        "print(f\"Transcription: {asr_result['transcription']}\")\n",
        "for h in asr_result[\"hypotheses\"]:\n",
        "    print(f\"  {h['score']:.4f}  {h['text']}\")\n",
        "\n",
        "# Multimodal\n",
        "mm_result = pipeline.transcribe(audio_array, image_id=image_id, sr=sr)\n",
        "print(f\"\\n--- Multimodal (mode: {mm_result['mode']}, \\u03b1={mm_result['alpha']}) ---\")\n",
        "print(f\"Transcription: {mm_result['transcription']}\")\n",
        "for h in mm_result[\"hypotheses\"]:\n",
        "    print(f\"  fused={h['fused_score']:.4f}  asr={h['asr_score']:.4f}  clip={h['clip_score']:.4f}  {h['text']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Examining the fusion in detail\n",
        "\n",
        "To understand how α affects the ranking, let's run the same hypotheses through\n",
        "`rescore` with several different α values and observe which hypothesis wins."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hypotheses = pipeline.generate_nbest(audio_array, sr)\n",
        "img_emb = pipeline.image_embeddings.get(image_id)\n",
        "\n",
        "if img_emb is not None:\n",
        "    for alpha in [0.0, 0.1, 0.3, 0.5, 0.7, 1.0]:\n",
        "        rescored = pipeline.rescore(hypotheses, img_emb, alpha=alpha)\n",
        "        best = rescored[0]\n",
        "        print(f\"  \\u03b1={alpha:.1f}  best=\\\"{best['text']}\\\"  fused={best['fused_score']:.4f}\")\n",
        "else:\n",
        "    print(f\"Image {image_id} not found in cache. Run clip_embeddings notebook first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook built the `MultimodalASR` class, the central piece of Phase 3:\n",
        "\n",
        "| Method | Purpose |\n",
        "|--------|--------|\n",
        "| `generate_nbest` | Whisper beam search → *n* unique hypotheses with log-prob scores |\n",
        "| `clip_similarity` | Cosine similarity between image embedding and text candidates |\n",
        "| `rescore` | Fuses ASR + CLIP probabilities with linear interpolation (α) |\n",
        "| `transcribe` | End-to-end: audio in → best transcription out |\n",
        "\n",
        "The fusion coefficient **α** controls the balance:\n",
        "- α = 0 → pure ASR (CLIP ignored)\n",
        "- α = 1 → pure CLIP (ASR confidence ignored)\n",
        "- α ≈ 0.2–0.4 is typically optimal (determined by grid search in `fusion_tuning`)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
