{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transcript Normalization for CLIP\n",
    "\n",
    "This notebook explains and demonstrates the **transcript normalization** step\n",
    "that prepares raw ASR output for CLIP's text encoder.\n",
    "\n",
    "## The problem\n",
    "\n",
    "CLIP was trained on **image-caption pairs** — clean, descriptive sentences like:\n",
    "\n",
    "> *\"a cat sleeping on a windowsill, feeling calm\"*\n",
    "\n",
    "But ASR output from dysarthric speakers often contains:\n",
    "- **Filler words**: *um, uh, like, you know*\n",
    "- **Stuttering / disfluency**: *the the cat is is sleeping*\n",
    "- **Fragments**: *cat* (a single word instead of a sentence)\n",
    "- **Inconsistent casing**: *A CAT sleeping*\n",
    "\n",
    "Feeding raw ASR output directly into CLIP produces poor similarity scores.\n",
    "Normalization bridges this gap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0) Imports\n",
    "\n",
    "Only `re` (regular expressions) is needed — this module is intentionally lightweight\n",
    "with no heavy dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Filler word list\n",
    "\n",
    "These are common filler words and discourse markers that appear in spontaneous speech\n",
    "but carry no semantic content relevant to image description. The list includes both\n",
    "**single-token** fillers (`um`, `uh`) and **two-token** phrases (`you know`, `i mean`).\n",
    "\n",
    "Using a `frozenset` for O(1) membership testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILLER_WORDS = frozenset({\n",
    "    \"um\", \"uh\", \"uh-huh\", \"hmm\", \"hm\", \"ah\", \"er\", \"oh\",\n",
    "    \"like\", \"you know\", \"i mean\", \"okay\", \"ok\", \"so\", \"well\",\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) `normalize_transcript` — core cleanup\n",
    "\n",
    "This function applies three transformations in order:\n",
    "\n",
    "1. **Lowercase + strip** — removes casing differences\n",
    "2. **Collapse stutters** — `\"the the cat is is sleeping\"` → `\"the cat is sleeping\"`  \n",
    "   Uses the regex `\\b(\\w+)( \\1\\b)+` to match any word repeated consecutively.\n",
    "3. **Remove fillers** — scans word-by-word, checking both single tokens and bigrams\n",
    "   against `FILLER_WORDS`\n",
    "\n",
    "The result is a clean, lowercase transcript with no disfluencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_transcript(text: str) -> str:\n",
    "    \"\"\"Clean an ASR transcript: lowercase, remove fillers, collapse stutters.\"\"\"\n",
    "    text = text.strip().lower()\n",
    "    if not text:\n",
    "        return text\n",
    "\n",
    "    text = re.sub(r\"\\b(\\w+)( \\1\\b)+\", r\"\\1\", text)\n",
    "\n",
    "    words = text.split()\n",
    "    cleaned: list[str] = []\n",
    "    skip_next = False\n",
    "    for i, w in enumerate(words):\n",
    "        if skip_next:\n",
    "            skip_next = False\n",
    "            continue\n",
    "        bigram = f\"{w} {words[i + 1]}\" if i + 1 < len(words) else \"\"\n",
    "        if bigram in FILLER_WORDS:\n",
    "            skip_next = True\n",
    "            continue\n",
    "        if w not in FILLER_WORDS:\n",
    "            cleaned.append(w)\n",
    "    text = \" \".join(cleaned)\n",
    "\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) `to_caption_style` — rephrasing for CLIP\n",
    "\n",
    "CLIP's text encoder works best with **full sentences** that describe an image. Very short\n",
    "ASR outputs (1–2 words) like `\"cat\"` produce weak embeddings because CLIP rarely saw\n",
    "isolated words during training.\n",
    "\n",
    "This function wraps short fragments as captions:\n",
    "- `\"cat\"` → `\"an image showing cat\"`\n",
    "- `\"a dog\"` → `\"an image showing a dog\"`\n",
    "- `\"the cat is sleeping on the window\"` → left unchanged (already sentence-length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_caption_style(text: str) -> str:\n",
    "    \"\"\"Rephrase a short transcript as a caption-like sentence for CLIP.\"\"\"\n",
    "    text = normalize_transcript(text)\n",
    "    if not text:\n",
    "        return text\n",
    "    if len(text.split()) <= 2:\n",
    "        return f\"an image showing {text}\"\n",
    "    return text\n",
    "\n",
    "\n",
    "def batch_normalize(\n",
    "    texts: list[str], caption_style: bool = False\n",
    ") -> list[str]:\n",
    "    \"\"\"Normalize a list of transcripts.\"\"\"\n",
    "    fn = to_caption_style if caption_style else normalize_transcript\n",
    "    return [fn(t) for t in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Demo: before and after normalization\n",
    "\n",
    "The examples below simulate realistic ASR outputs and show how each normalization\n",
    "stage transforms them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    \"Uh the the cat is is sleeping on the window\",\n",
    "    \"um like a dog\",\n",
    "    \"cat\",\n",
    "    \"  A BOY riding a bicycle on a PATH  \",\n",
    "    \"well you know the the girl is painting\",\n",
    "    \"\",\n",
    "]\n",
    "\n",
    "for raw in examples:\n",
    "    norm = normalize_transcript(raw)\n",
    "    cap = to_caption_style(raw)\n",
    "    print(f\"  raw:     {raw!r}\")\n",
    "    print(f\"  norm:    {norm!r}\")\n",
    "    print(f\"  caption: {cap!r}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Step | What it does | Example |\n",
    "|------|-------------|--------|\n",
    "| Lowercase + strip | Removes casing noise | `\"A CAT\"` → `\"a cat\"` |\n",
    "| Collapse stutters | Deduplicates repeated words | `\"the the cat\"` → `\"the cat\"` |\n",
    "| Remove fillers | Strips discourse markers | `\"uh like a dog\"` → `\"a dog\"` |\n",
    "| Caption wrapping | Pads short fragments | `\"cat\"` → `\"an image showing cat\"` |\n",
    "\n",
    "This normalization is applied automatically inside the multimodal rescoring pipeline\n",
    "(`multimodal_asr.py`) before text is sent to CLIP's text encoder."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
